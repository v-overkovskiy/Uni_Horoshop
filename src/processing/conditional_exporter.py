"""
–£—Å–ª–æ–≤–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä - –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω–æ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
"""
import os
import pandas as pd
import logging
import re
from datetime import datetime
from typing import Dict, Any, List

logger = logging.getLogger(__name__)

class ConditionalExporter:
    """–≠–∫—Å–ø–æ—Ä—Ç–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
    
    def __init__(self, output_file: str = "descriptions.xlsx", mode: str = "paired_always"):
        self.output_file = output_file
        self.results = []
        self.repair_report = []
        self.mode = mode  # "paired_always" –∏–ª–∏ "strict"
        
        # –ë—É—Ñ–µ—Ä –¥–ª—è —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º
        self.ordered_buffer = {}  # Map<index, RowData>
        self.input_urls = []  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ URL
        self.input_count = 0
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
        self.specs_clamped_count = 0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–µ—á–µ–Ω–Ω—ã—Ö –∫–∞—Ä—Ç–æ—á–µ–∫
        self.total_specs_processed = 0  # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        self.specs_priority_violations = 0  # –ù–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è note-buy –∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        self.note_buy_sanitized_count = 0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞–Ω–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö note-buy
        self.note_buy_source_stats = {'original': 0, 'safe_constructor': 0, 'failed_sanitization': 0}
        self.title_sanitized_count = 0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∞–Ω–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        self.title_source_stats = {'original': 0, 'safe_constructor': 0, 'failed_sanitization': 0}
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã —Ä–µ–º–æ–Ω—Ç–∞
        self.repair_enqueued_count = 0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –≤ —Ä–µ–º–æ–Ω—Ç
        self.repair_completed_count = 0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Å–ø–µ—à–Ω–æ –æ—Ç—Ä–µ–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö URL
        self.repair_failed_count = 0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ—É–¥–∞—á–Ω–æ –æ—Ç—Ä–µ–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö URL
        self.sanity_fix_applied_count = 0  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã—Ö sanity-—Ñ–∏–∫—Å–æ–≤
        self.repair_reason_stats = {}  # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏—á–∏–Ω–∞–º —Ä–µ–º–æ–Ω—Ç–∞
        # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –¥–æ–ª–∂–Ω—ã –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é
        self.neutral_whitelist = {
            'ru': ['—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–º–∞–∫—Å–∏–º—É–º', '–º–∏–Ω–∏–º—É–º', '–æ–ø—Ç–∏–º—É–º', '–ø—Ä–æ–¥—É–∫—Ç', '–º–∞—Ç–µ—Ä–∏–∞–ª'],
            'ua': ['—Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–º–∞–∫—Å–∏–º—É–º', '–º—ñ–Ω—ñ–º—É–º', '–æ–ø—Ç–∏–º—É–º', '–ø—Ä–æ–¥—É–∫—Ç', '–º–∞—Ç–µ—Ä—ñ–∞–ª']
        }
    
    def initialize_with_urls(self, urls: List[str]) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä —Å –≤—Ö–æ–¥–Ω—ã–º–∏ URL –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞"""
        self.input_urls = urls.copy()
        self.input_count = len(urls)
        self.ordered_buffer = {}
        logger.info(f"üìã –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä —Å {self.input_count} URL")
    
    def add_result_by_index(self, result: Dict[str, Any], index: int) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ –∏–Ω–¥–µ–∫—Å—É –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Å—Ç—Ä–æ–≥–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞"""
        if index < 1 or index > self.input_count:
            logger.error(f"‚ùå –ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –∏–Ω–¥–µ–∫—Å {index}, –æ–∂–∏–¥–∞–µ—Ç—Å—è 1-{self.input_count}")
            return
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ª–æ–∫–∞–ª–µ–π
        ru_valid = self._is_locale_valid(result, 'ru')
        ua_valid = self._is_locale_valid(result, 'ua')
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –∑–Ω–∞—á–µ–Ω–∏—è—Ö
        normalized_result = self._normalize_spaces_in_values(result)
        
        if self.mode == "paired_always":
            # –†–µ–∂–∏–º "–≤—Å–µ–≥–¥–∞ –ø–∞—Ä–∞" - –≤—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–¥–Ω–∞ –ª–æ–∫–∞–ª—å –ø—Ä–æ–±–ª–µ–º–Ω–∞—è
            if ru_valid and ua_valid:
                # –û–±–µ –ª–æ–∫–∞–ª–∏ –≤–∞–ª–∏–¥–Ω—ã - –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                row_data = self._create_row_data(normalized_result, ru_valid, ua_valid)
                self.ordered_buffer[index] = row_data
                logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –≤–∞–ª–∏–¥–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ –∏–Ω–¥–µ–∫—Å—É {index}: {result.get('url', 'unknown')}")
            else:
                # –ï—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã - –ø—ã—Ç–∞–µ–º—Å—è —Å–∞–Ω–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å
                sanitized_result = self._sanitize_result(normalized_result, ru_valid, ua_valid)
                if sanitized_result:
                    row_data = self._create_row_data(sanitized_result, ru_valid, ua_valid)
                    self.ordered_buffer[index] = row_data
                    logger.info(f"üîß –î–æ–±–∞–≤–ª–µ–Ω —Å–∞–Ω–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ –∏–Ω–¥–µ–∫—Å—É {index}: {result.get('url', 'unknown')}")
                else:
                    # –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –Ω–µ –ø–æ–º–æ–≥–ª–∞ - –¥–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    partial_result = self._create_partial_result(normalized_result, ru_valid, ua_valid)
                    row_data = self._create_row_data(partial_result, ru_valid, ua_valid)
                    self.ordered_buffer[index] = row_data
                    logger.warning(f"‚ö†Ô∏è –î–æ–±–∞–≤–ª–µ–Ω —á–∞—Å—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ –∏–Ω–¥–µ–∫—Å—É {index}: {result.get('url', 'unknown')}")
                
                # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ repair_report –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                self._add_to_repair_report(normalized_result, ru_valid, ua_valid)
        else:
            # –°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º - —Ç–æ–ª—å–∫–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤–∞–ª–∏–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if ru_valid and ua_valid:
                row_data = self._create_row_data(normalized_result, ru_valid, ua_valid)
                self.ordered_buffer[index] = row_data
                logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –≤–∞–ª–∏–¥–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ –∏–Ω–¥–µ–∫—Å—É {index}: {result.get('url', 'unknown')}")
            else:
                self._add_to_repair_report(normalized_result, ru_valid, ua_valid)
    
    def add_result(self, result: Dict[str, Any]) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —É—á–µ—Ç–æ–º —Ä–µ–∂–∏–º–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ª–æ–∫–∞–ª–µ–π
        ru_valid = self._is_locale_valid(result, 'ru')
        ua_valid = self._is_locale_valid(result, 'ua')
        
        if self.mode == "paired_always":
            # –†–µ–∂–∏–º "–≤—Å–µ–≥–¥–∞ –ø–∞—Ä–∞" - –≤—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É, –¥–∞–∂–µ –µ—Å–ª–∏ –æ–¥–Ω–∞ –ª–æ–∫–∞–ª—å –ø—Ä–æ–±–ª–µ–º–Ω–∞—è
            if ru_valid and ua_valid:
                # –û–±–µ –ª–æ–∫–∞–ª–∏ –≤–∞–ª–∏–¥–Ω—ã - –¥–æ–±–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                normalized_result = self._normalize_result(result)
                self.results.append(normalized_result)
                logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –≤–∞–ª–∏–¥–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result.get('url', 'unknown')}")
            else:
                # –ï—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã - –ø—ã—Ç–∞–µ–º—Å—è —Å–∞–Ω–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å
                sanitized_result = self._sanitize_result(result, ru_valid, ua_valid)
                if sanitized_result:
                    normalized_result = self._normalize_result(sanitized_result)
                    self.results.append(normalized_result)
                    logger.info(f"üîß –î–æ–±–∞–≤–ª–µ–Ω —Å–∞–Ω–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result.get('url', 'unknown')}")
                else:
                    # –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –Ω–µ –ø–æ–º–æ–≥–ª–∞ - –¥–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    partial_result = self._create_partial_result(result, ru_valid, ua_valid)
                    normalized_result = self._normalize_result(partial_result)
                    self.results.append(normalized_result)
                    logger.warning(f"‚ö†Ô∏è –î–æ–±–∞–≤–ª–µ–Ω —á–∞—Å—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result.get('url', 'unknown')}")
                
                # –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ repair_report –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                self._add_to_repair_report(result, ru_valid, ua_valid)
        else:
            # –°—Ç—Ä–æ–≥–∏–π —Ä–µ–∂–∏–º - —Ç–æ–ª—å–∫–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –≤–∞–ª–∏–¥–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if ru_valid and ua_valid:
                normalized_result = self._normalize_result(result)
                self.results.append(normalized_result)
                logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –≤–∞–ª–∏–¥–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result.get('url', 'unknown')}")
            else:
                self._add_to_repair_report(result, ru_valid, ua_valid)
    
    def _is_locale_valid(self, result: Dict[str, Any], locale: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –≤–∞–ª–∏–¥–Ω–∞ –ª–∏ –ª–æ–∫–∞–ª—å"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥–∏ –æ—à–∏–±–æ–∫
        flags = result.get('flags', [])
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –æ—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è —ç—Ç–æ–π –ª–æ–∫–∞–ª–∏ - –Ω–µ–≤–∞–ª–∏–¥–Ω–∞
        for flag in flags:
            if f'Locale mixing detected in specs ({locale})' in flag:
                return False
            if f'LLM –≤–µ—Ä–Ω—É–ª –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π JSON –¥–ª—è {locale}' in flag:
                return False
            if f'ValidationError' in flag and locale in flag:
                return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        html_key = f'{locale.upper()}_HTML'
        html_content = result.get(html_key, '')
        
        if not html_content or html_content.strip() == '':
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ HTML —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Å–Ω–æ–≤–Ω—ã–µ –±–ª–æ–∫–∏
        if '<h2 class="prod-title">' not in html_content:
            return False
        
        return True
    
    def _sanitize_result(self, result: Dict[str, Any], ru_valid: bool, ua_valid: bool) -> Dict[str, Any]:
        """–ü—ã—Ç–∞–µ—Ç—Å—è —Å–∞–Ω–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
        sanitized = result.copy()
        
        # –ï—Å–ª–∏ UA –Ω–µ–≤–∞–ª–∏–¥–Ω–∞ - –ø—ã—Ç–∞–µ–º—Å—è —Å–∞–Ω–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å
        if not ua_valid:
            ua_html = result.get('UA_HTML', '')
            if ua_html:
                sanitized_ua = self._sanitize_html_content(ua_html, 'ua')
                if sanitized_ua != ua_html:
                    sanitized['UA_HTML'] = sanitized_ua
                    logger.info(f"üîß –°–∞–Ω–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω UA HTML –¥–ª—è {result.get('url', 'unknown')}")
        
        # –ï—Å–ª–∏ RU –Ω–µ–≤–∞–ª–∏–¥–Ω–∞ - –ø—ã—Ç–∞–µ–º—Å—è —Å–∞–Ω–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å
        if not ru_valid:
            ru_html = result.get('RU_HTML', '')
            if ru_html:
                sanitized_ru = self._sanitize_html_content(ru_html, 'ru')
                if sanitized_ru != ru_html:
                    sanitized['RU_HTML'] = sanitized_ru
                    logger.info(f"üîß –°–∞–Ω–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω RU HTML –¥–ª—è {result.get('url', 'unknown')}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–º–æ–≥–ª–∞ –ª–∏ —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è
        ru_valid_after = self._is_locale_valid(sanitized, 'ru')
        ua_valid_after = self._is_locale_valid(sanitized, 'ua')
        
        if ru_valid_after and ua_valid_after:
            return sanitized
        else:
            return None
    
    def _sanitize_html_content(self, html_content: str, locale: str) -> str:
        """–°–∞–Ω–∏—Ç–∏–∑–∏—Ä—É–µ—Ç HTML –∫–æ–Ω—Ç–µ–Ω—Ç, —É–¥–∞–ª—è—è –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–ª–æ–≤–∞"""
        if not html_content:
            return html_content
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –¥–ª—è —ç—Ç–æ–π –ª–æ–∫–∞–ª–∏
        neutral_words = self.neutral_whitelist.get(locale, [])
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–ª–æ–≤ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
        description_patterns = [
            r'<p class="description">(.*?)</p>',
            r'<div class="description">(.*?)</div>'
        ]
        
        sanitized = html_content
        
        for pattern in description_patterns:
            def replace_description(match):
                description = match.group(1)
                # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ
                for word in neutral_words:
                    # –ò—â–µ–º —Å–ª–æ–≤–æ –≤ —Ä–∞–∑–Ω—ã—Ö –ø–∞–¥–µ–∂–∞—Ö
                    word_pattern = re.compile(r'\b' + re.escape(word) + r'\b', re.IGNORECASE)
                    description = word_pattern.sub(word, description)
                return f'<p class="description">{description}</p>'
            
            sanitized = re.sub(pattern, replace_description, sanitized, flags=re.DOTALL)
        
        return sanitized
    
    def _create_partial_result(self, result: Dict[str, Any], ru_valid: bool, ua_valid: bool) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç —á–∞—Å—Ç–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –≤–∞–ª–∏–¥–Ω—ã–º–∏ –±–ª–æ–∫–∞–º–∏"""
        partial = result.copy()
        
        # –ï—Å–ª–∏ UA –Ω–µ–≤–∞–ª–∏–¥–Ω–∞ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –±–ª–æ–∫–∏
        if not ua_valid:
            ua_html = result.get('UA_HTML', '')
            if ua_html:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∞–ª–∏–¥–Ω—ã–µ –±–ª–æ–∫–∏ (note-buy, specs, FAQ, hero)
                valid_blocks = self._extract_valid_blocks(ua_html)
                partial['UA_HTML'] = valid_blocks
                logger.info(f"üîß –°–æ–∑–¥–∞–Ω —á–∞—Å—Ç–∏—á–Ω—ã–π UA HTML –¥–ª—è {result.get('url', 'unknown')}")
        
        # –ï—Å–ª–∏ RU –Ω–µ–≤–∞–ª–∏–¥–Ω–∞ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –±–ª–æ–∫–∏
        if not ru_valid:
            ru_html = result.get('RU_HTML', '')
            if ru_html:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∞–ª–∏–¥–Ω—ã–µ –±–ª–æ–∫–∏ (note-buy, specs, FAQ, hero)
                valid_blocks = self._extract_valid_blocks(ru_html)
                partial['RU_HTML'] = valid_blocks
                logger.info(f"üîß –°–æ–∑–¥–∞–Ω —á–∞—Å—Ç–∏—á–Ω—ã–π RU HTML –¥–ª—è {result.get('url', 'unknown')}")
        
        return partial
    
    def _extract_valid_blocks(self, html_content: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ –±–ª–æ–∫–∏ –∏–∑ HTML"""
        if not html_content:
            return html_content
        
        valid_blocks = []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º note-buy
        note_buy_match = re.search(r'<p class="note-buy">(.*?)</p>', html_content, re.DOTALL)
        if note_buy_match:
            valid_blocks.append(note_buy_match.group(0))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º specs
        specs_match = re.search(r'<ul class="specs">(.*?)</ul>', html_content, re.DOTALL)
        if specs_match:
            valid_blocks.append(specs_match.group(0))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º FAQ
        faq_match = re.search(r'<div class="faq">(.*?)</div>', html_content, re.DOTALL)
        if faq_match:
            valid_blocks.append(faq_match.group(0))
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º hero
        hero_match = re.search(r'<figure class="hero">(.*?)</figure>', html_content, re.DOTALL)
        if hero_match:
            valid_blocks.append(hero_match.group(0))
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if valid_blocks:
            return '\n'.join(valid_blocks)
        else:
            return html_content
    
    def _add_to_repair_report(self, result: Dict[str, Any], ru_valid: bool, ua_valid: bool) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ repair_report"""
        repair_entry = {
            'url': result.get('url', 'unknown'),
            'ru_valid': ru_valid,
            'ua_valid': ua_valid,
            'flags': result.get('flags', []),
            'export_mode': self._determine_export_mode(result),
            'needs_review': result.get('needs_review', True),
            'timestamp': datetime.now().isoformat()
        }
        
        self.repair_report.append(repair_entry)

    def _determine_export_mode(self, result: Dict[str, Any]) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–∂–∏–º —ç–∫—Å–ø–æ—Ä—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω —Ä–µ–º–æ–Ω—Ç
        if result.get('repair_applied', False):
            return 'repair'
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ repair_report (–∑–Ω–∞—á–∏—Ç –±—ã–ª —Ä–µ–º–æ–Ω—Ç)
        if hasattr(self, 'repair_report') and self.repair_report:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç–µ–∫—É—â–∏–π URL –≤ repair_report
            current_url = result.get('url', '')
            for repair_item in self.repair_report:
                if repair_item.get('url') == current_url:
                    return 'repair'
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∂–∏–º –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        export_mode = result.get('export_mode', '')
        if export_mode in ['full', 'fallback', 'specs_only', 'safe_full']:
            return export_mode
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∫–ª—é—á–∏
        ru_valid = result.get('RU_Valid', False)
        ua_valid = result.get('UA_Valid', False)
        
        if ru_valid and ua_valid:
            return 'normal'
        elif ru_valid or ua_valid:
            return 'partial'
        else:
            return 'failed'
    
    def _normalize_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ª–æ–∫–∞–ª–µ–π
        ru_valid = self._is_locale_valid(result, 'ru')
        ua_valid = self._is_locale_valid(result, 'ua')
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã
        ru_issues = []
        ua_issues = []
        
        flags = result.get('flags', [])
        for flag in flags:
            if 'ru' in flag.lower() and 'validation' in flag.lower():
                ru_issues.append(flag)
            elif 'ua' in flag.lower() and 'validation' in flag.lower():
                ua_issues.append(flag)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è
        diagnostic_fields = self._extract_diagnostic_fields(result, ru_valid, ua_valid)
        
        # –ë–∞–∑–æ–≤—ã–µ –ø–æ–ª—è
        base_fields = {
            'URL': result.get('url', ''),
            'RU_HTML': result.get('RU_HTML', ''),
            'UA_HTML': result.get('UA_HTML', ''),
            'RU_Valid': ru_valid,
            'UA_Valid': ua_valid,
            'RU_Issues': '; '.join(ru_issues) if ru_issues else '',
            'UA_Issues': '; '.join(ua_issues) if ua_issues else ''
        }
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±–∞–∑–æ–≤—ã–µ –ø–æ–ª—è —Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–º–∏
        return {**base_fields, **diagnostic_fields}
    
    def _extract_diagnostic_fields(self, result: Dict[str, Any], ru_valid: bool, ua_valid: bool) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è –¥–ª—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ —Ä–µ–º–æ–Ω—Ç–∞"""
        diagnostic = {}
        
        # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        diagnostic['source_title_ru_snapshot'] = result.get('source_title_ru_snapshot', '')
        diagnostic['source_title_ua_snapshot'] = result.get('source_title_ua_snapshot', '')
        
        # H2 –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        ru_html = result.get('RU_HTML', '')
        ua_html = result.get('UA_HTML', '')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º h2 –∏–∑ HTML
        ru_h2 = self._extract_h2_from_html(ru_html)
        ua_h2 = self._extract_h2_from_html(ua_html)
        
        diagnostic['h2_ru_before'] = ru_h2
        diagnostic['h2_ru_after'] = ru_h2  # –ü–æ—Å–ª–µ —Ä–µ–º–æ–Ω—Ç–∞ –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ
        diagnostic['h2_ru_source'] = self._determine_h2_source(ru_h2, result, 'ru')
        diagnostic['h2_ua_before'] = ua_h2
        diagnostic['h2_ua_after'] = ua_h2
        
        # –°—Ç–∞—Ç—É—Å RU HTML
        diagnostic['ru_html_empty'] = not ru_html or len(ru_html.strip()) < 10
        diagnostic['ru_h2_len'] = len(ru_h2) if ru_h2 else 0
        diagnostic['ru_description_paragraphs'] = self._count_description_paragraphs(ru_html)
        diagnostic['ru_description_chars'] = self._count_description_chars(ru_html)
        
        # Locale-mixing –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        diagnostic['ru_locale_mixing_detected'] = self._detect_locale_mixing(ru_html, 'ru')
        diagnostic['ru_mixing_reason'] = self._get_mixing_reason(ru_html, 'ru')
        diagnostic['ru_has_ua_letters'] = self._has_ua_letters(ru_html)
        diagnostic['ru_mixing_tokens'] = self._extract_mixing_tokens(ru_html, 'ru')
        diagnostic['ru_whitelist_hit'] = self._check_whitelist_hit(ru_html, 'ru')
        
        # Specs –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        ru_specs = self._extract_specs_from_html(ru_html)
        ua_specs = self._extract_specs_from_html(ua_html)
        
        diagnostic['specs_count_ru'] = len(ru_specs)
        diagnostic['specs_keys_ru'] = [spec.get('name', '') for spec in ru_specs]
        diagnostic['specs_conflicts_dropped'] = self._get_dropped_conflicts(ru_specs, 'ru')
        diagnostic['specs_fixed_labels'] = self._get_fixed_labels(ru_specs, 'ru')
        
        # –í–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–µ—Ä–µ–¥ —ç–∫—Å–ø–æ—Ä—Ç–æ–º
        diagnostic['faq_jsonld_valid_ru'] = self._validate_faq_jsonld(ru_html)
        diagnostic['note_buy_status_ru'] = self._get_note_buy_status(ru_html)
        diagnostic['product_photo_present_ru'] = self._has_product_photo(ru_html)
        diagnostic['export_mode_final'] = self._determine_export_mode(result)
        diagnostic['repair_scope'] = self._get_repair_scope(result)
        diagnostic['repair_actions_applied'] = self._get_repair_actions(result)
        diagnostic['repair_failure_reason'] = self._get_repair_failure_reason(result)
        
        # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ FAQ –∏ note_buy
        enhancement_diagnostic = result.get('enhancement_diagnostic', {})
        diagnostic.update(self._extract_enhancement_diagnostic(enhancement_diagnostic, ru_html, ua_html))
        
        return diagnostic
    
    def _extract_enhancement_diagnostic(self, enhancement_diagnostic: Dict[str, Any], ru_html: str, ua_html: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —É–ª—É—á—à–µ–Ω–∏—è—Ö FAQ –∏ note_buy"""
        diagnostic = {}
        
        # FAQ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        diagnostic['faq_q_lowercase_count'] = enhancement_diagnostic.get('faq_q_lowercase_count', 0)
        diagnostic['faq_unit_mismatch_count'] = enhancement_diagnostic.get('faq_unit_mismatch_count', 0)
        diagnostic['faq_weight_stub_count'] = enhancement_diagnostic.get('faq_weight_stub_count', 0)
        diagnostic['faq_first_slot_repaired'] = enhancement_diagnostic.get('faq_first_slot_repaired', False)
        diagnostic['faq_repaired'] = enhancement_diagnostic.get('faq_repaired', False)
        diagnostic['faq_repair_actions'] = str(enhancement_diagnostic.get('faq_repair_actions', []))
        diagnostic['faq_candidates_total'] = enhancement_diagnostic.get('faq_candidates_total', 0)
        diagnostic['faq_selected_count'] = enhancement_diagnostic.get('faq_selected_count', 0)
        diagnostic['faq_topics_covered'] = enhancement_diagnostic.get('topics_covered', 0)
        
        # Note-buy –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        diagnostic['note_buy_has_kupit_kupyty'] = enhancement_diagnostic.get('note_buy_has_kupit_kupyty', False)
        diagnostic['note_buy_declined'] = enhancement_diagnostic.get('note_buy_declined', False)
        diagnostic['note_buy_single_strong'] = enhancement_diagnostic.get('note_buy_single_strong', False)
        diagnostic['note_buy_range_from'] = enhancement_diagnostic.get('note_buy_range_from', '')
        diagnostic['note_buy_range_to'] = enhancement_diagnostic.get('note_buy_range_to', '')
        diagnostic['note_buy_first_char_lowered'] = enhancement_diagnostic.get('note_buy_first_char_lowered', False)
        diagnostic['note_buy_before'] = enhancement_diagnostic.get('note_buy_before', '')
        diagnostic['note_buy_after'] = enhancement_diagnostic.get('note_buy_after', '')
        
        # –°–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        declension_debug = enhancement_diagnostic.get('declension_debug', {})
        diagnostic['declension_debug_first_adj'] = declension_debug.get('first_adj', '')
        diagnostic['declension_debug_first_noun'] = declension_debug.get('first_noun', '')
        diagnostic['declension_debug_rules_applied'] = str(declension_debug.get('rules_applied', []))
        
        # Lowercase –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        lowercase_debug = enhancement_diagnostic.get('lowercase_debug', {})
        diagnostic['lowercase_debug_position'] = lowercase_debug.get('position', -1)
        diagnostic['lowercase_debug_original_char'] = lowercase_debug.get('original_char', '')
        diagnostic['lowercase_debug_lowercased_char'] = lowercase_debug.get('lowercased_char', '')
        
        return diagnostic
    
    def _extract_h2_from_html(self, html: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç h2 –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ HTML"""
        if not html:
            return ''
        
        import re
        h2_match = re.search(r'<h2[^>]*class="prod-title"[^>]*>(.*?)</h2>', html, re.DOTALL)
        if h2_match:
            return h2_match.group(1).strip()
        return ''
    
    def _determine_h2_source(self, h2: str, result: Dict[str, Any], locale: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫ h2 –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        if not h2:
            return 'none'
        
        ru_snapshot = result.get('source_title_ru_snapshot', '')
        ua_snapshot = result.get('source_title_ua_snapshot', '')
        
        if h2 == ru_snapshot:
            return 'ru_snapshot'
        elif h2 == ua_snapshot:
            return 'ua_fallback'
        else:
            return 'generated'
    
    def _count_description_paragraphs(self, html: str) -> int:
        """–°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–±–∑–∞—Ü–µ–≤ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏"""
        if not html:
            return 0
        
        import re
        # –ò—â–µ–º –∞–±–∑–∞—Ü—ã –≤ —Å–µ–∫—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è
        description_match = re.search(r'<h2[^>]*>–û–ø–∏—Å–∞–Ω–∏–µ</h2>(.*?)(?=<h2|$)', html, re.DOTALL)
        if description_match:
            p_matches = re.findall(r'<p[^>]*>(.*?)</p>', description_match.group(1), re.DOTALL)
            return len([p for p in p_matches if p.strip()])
        return 0
    
    def _count_description_chars(self, html: str) -> int:
        """–°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏"""
        if not html:
            return 0
        
        import re
        # –ò—â–µ–º —Ç–µ–∫—Å—Ç –≤ —Å–µ–∫—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è
        description_match = re.search(r'<h2[^>]*>–û–ø–∏—Å–∞–Ω–∏–µ</h2>(.*?)(?=<h2|$)', html, re.DOTALL)
        if description_match:
            # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏ –∏ —Å—á–∏—Ç–∞–µ–º —Å–∏–º–≤–æ–ª—ã
            text = re.sub(r'<[^>]+>', '', description_match.group(1))
            return len(text.strip())
        return 0
    
    def _detect_locale_mixing(self, html: str, locale: str) -> bool:
        """–î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç —Å–º–µ—à–µ–Ω–∏–µ –ª–æ–∫–∞–ª–µ–π –≤ HTML"""
        if not html:
            return False
        
        if locale == 'ru':
            # –ò—â–µ–º UA-–±—É–∫–≤—ã —Å –≥—Ä–∞–Ω–∏—Ü–∞–º–∏ —Å–ª–æ–≤
            import re
            ua_letters_pattern = r'\b[—ñ—ó—î“ë–Ü–á–Ñ“ê]\w*\b'
            return bool(re.search(ua_letters_pattern, html))
        return False
    
    def _get_mixing_reason(self, html: str, locale: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—Ä–∏—á–∏–Ω—É —Å–º–µ—à–µ–Ω–∏—è –ª–æ–∫–∞–ª–µ–π"""
        if not html:
            return ''
        
        if locale == 'ru':
            import re
            if re.search(r'\b[—ñ—ó—î“ë–Ü–á–Ñ“ê]\w*\b', html):
                return 'ua_letters'
            elif re.search(r'\b(–≥–∞—Ä—è—á–∏–π|–æ–±–ª–∏—á—á—è|–æ–±–ª–∞—Å—Ç—å –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è)\b', html, re.IGNORECASE):
                return 'ua_tokens'
        return ''
    
    def _has_ua_letters(self, html: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ UA-–±—É–∫–≤ –≤ HTML"""
        if not html:
            return False
        
        import re
        return bool(re.search(r'[—ñ—ó—î“ë–Ü–á–Ñ“ê]', html))
    
    def _extract_mixing_tokens(self, html: str, locale: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–æ–∫–µ–Ω—ã —Å–º–µ—à–µ–Ω–∏—è –ª–æ–∫–∞–ª–µ–π"""
        if not html:
            return ''
        
        import re
        if locale == 'ru':
            ua_tokens = re.findall(r'\b[—ñ—ó—î“ë–Ü–á–Ñ“ê]\w*\b', html)
            return ', '.join(ua_tokens[:5])  # –ü–µ—Ä–≤—ã–µ 5 —Ç–æ–∫–µ–Ω–æ–≤
        return ''
    
    def _check_whitelist_hit(self, html: str, locale: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ–ø–∞–¥–∞–Ω–∏–µ –≤ whitelist"""
        if not html:
            return False
        
        whitelist_words = ['–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è', '–∫–ª–∞—Å—Å', '–∫–ª–∞—Å']
        for word in whitelist_words:
            if word.lower() in html.lower():
                return True
        return False
    
    def _extract_specs_from_html(self, html: str) -> List[Dict[str, str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏–∑ HTML"""
        if not html:
            return []
        
        import re
        specs = []
        specs_match = re.search(r'<ul class="specs">(.*?)</ul>', html, re.DOTALL)
        if specs_match:
            li_matches = re.findall(r'<li[^>]*>(.*?)</li>', specs_match.group(1), re.DOTALL)
            for li in li_matches:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º name –∏ value
                span_match = re.search(r'<span[^>]*class="spec-label"[^>]*>(.*?):</span>\s*(.*)', li, re.DOTALL)
                if span_match:
                    specs.append({
                        'name': span_match.group(1).strip(),
                        'value': span_match.group(2).strip()
                    })
        return specs
    
    def _get_dropped_conflicts(self, specs: List[Dict[str, str]], locale: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã—Ö –∫–ª—é—á–µ–π"""
        if not specs:
            return ''
        
        conflict_patterns = {
            'ru': ['–≥–∞—Ä—è—á–∏–π', '–æ–±–ª–∏—á—á—è', '–æ–±–ª–∞—Å—Ç—å –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è'],
            'ua': ['–≥–æ—Ä—è—á–∏–π', '–ª–∏—Ü–µ', '–æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è']
        }
        
        patterns = conflict_patterns.get(locale, [])
        dropped = []
        for spec in specs:
            name = spec.get('name', '').lower()
            for pattern in patterns:
                if pattern in name:
                    dropped.append(spec.get('name', ''))
                    break
        
        return ', '.join(dropped)
    
    def _get_fixed_labels(self, specs: List[Dict[str, str]], locale: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –ª–µ–π–±–ª–æ–≤"""
        # –≠—Ç–æ –±—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω—è—Ç—å—Å—è –≤ repair_runner –ø—Ä–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        return ''
    
    def _validate_faq_jsonld(self, html: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å FAQ JSON-LD"""
        if not html:
            return False
        
        import re
        script_match = re.search(r'<script[^>]*data-prorazko="faq"[^>]*>(.*?)</script>', html, re.DOTALL)
        if script_match:
            try:
                import json
                json.loads(script_match.group(1))
                return True
            except:
                return False
        return False
    
    def _get_note_buy_status(self, html: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å note-buy"""
        if not html:
            return 'missing'
        
        import re
        note_buy_match = re.search(r'<p class="note-buy">(.*?)</p>', html, re.DOTALL)
        if note_buy_match:
            content = note_buy_match.group(1).strip()
            if content and content != '<strong>–∫—É–ø–∏—Ç—å </strong>' and content != '<strong>–∫—É–ø–∏—Ç–∏ </strong>':
                return 'normalized'
            else:
                return 'original'
        return 'missing'
    
    def _has_product_photo(self, html: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ñ–æ—Ç–æ —Ç–æ–≤–∞—Ä–∞"""
        if not html:
            return False
        
        import re
        return bool(re.search(r'<img[^>]*src="[^"]*"[^>]*>', html))
    
    def _get_repair_scope(self, result: Dict[str, Any]) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ–±–ª–∞—Å—Ç—å —Ä–µ–º–æ–Ω—Ç–∞"""
        flags = result.get('flags', [])
        if any('repair' in flag.lower() for flag in flags):
            return 'repair_applied'
        return 'main_flow'
    
    def _get_repair_actions(self, result: Dict[str, Any]) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è —Ä–µ–º–æ–Ω—Ç–∞"""
        flags = result.get('flags', [])
        repair_actions = [flag for flag in flags if 'repair' in flag.lower()]
        return '; '.join(repair_actions) if repair_actions else ''
    
    def _get_repair_failure_reason(self, result: Dict[str, Any]) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–∏—á–∏–Ω—É –Ω–µ—É–¥–∞—á–∏ —Ä–µ–º–æ–Ω—Ç–∞"""
        flags = result.get('flags', [])
        failure_flags = [flag for flag in flags if 'failed' in flag.lower() or 'error' in flag.lower()]
        return '; '.join(failure_flags) if failure_flags else ''
    
    def _normalize_spaces_in_values(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤ –∑–Ω–∞—á–µ–Ω–∏—è—Ö, —Å–æ—Ö—Ä–∞–Ω—è—è –∏—Å—Ö–æ–¥–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ —è–∑—ã–∫"""
        normalized = result.copy()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–µ
        for locale in ['ru', 'ua']:
            html_key = f'{locale.upper()}_HTML'
            if html_key in normalized:
                html_content = normalized[html_key]
                if html_content:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –∑–Ω–∞—á–µ–Ω–∏—è—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
                    normalized_html = self._normalize_html_spaces(html_content)
                    if normalized_html != html_content:
                        normalized[html_key] = normalized_html
                        logger.debug(f"üîß –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –ø—Ä–æ–±–µ–ª—ã –≤ {html_key}")
        
        return normalized
    
    def _normalize_html_spaces(self, html_content: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤ HTML –∫–æ–Ω—Ç–µ–Ω—Ç–µ –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è"""
        if not html_content:
            return html_content
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã –≤ –∑–Ω–∞—á–µ–Ω–∏—è—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        def normalize_spec_value(match):
            name = match.group(1).strip()
            value = match.group(2).strip()
            
            # –ï–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ values: split/trim/join —á–µ—Ä–µ–∑ ¬´, ¬ª
            if ',' in value:
                tokens = [token.strip() for token in value.split(',') if token.strip()]
                value = ', '.join(tokens)  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —Å –ø—Ä–æ–±–µ–ª–æ–º
            
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ name
            name = ' '.join(name.split())
            
            return f'<li><span class="spec-label">{name}:</span> {value}</li>'
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫: <li><span class="spec-label">Name:</span> Value</li>
        pattern = r'<li><span class="spec-label">([^:]+):</span>\s*([^<]+)</li>'
        normalized = re.sub(pattern, normalize_spec_value, html_content, flags=re.DOTALL)
        
        return normalized
    
    def _create_row_data(self, result: Dict[str, Any], ru_valid: bool, ua_valid: bool) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω–æ–≥–æ —ç–∫—Å–ø–æ—Ä—Ç–∞"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã
        ru_issues = []
        ua_issues = []
        
        flags = result.get('flags', [])
        for flag in flags:
            if 'ru' in flag.lower() and 'validation' in flag.lower():
                ru_issues.append(flag)
            elif 'ua' in flag.lower() and 'validation' in flag.lower():
                ua_issues.append(flag)
        
        # –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–í–ï–†–ö–ò –ò –ú–ï–¢–†–ò–ö–ò
        self._update_specs_metrics(result)
        self._update_note_buy_metrics(result)
        self._update_title_metrics(result)
        
        # –°–æ–∑–¥–∞–µ–º —Ö–µ—à –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
        source_hash = self._calculate_source_hash(result)
        
        return {
            'URL': result.get('url', ''),
            'RU_HTML': result.get('RU_HTML', ''),
            'UA_HTML': result.get('UA_HTML', ''),
            'RU_Valid': ru_valid,
            'UA_Valid': ua_valid,
            'RU_Issues': '; '.join(ru_issues) if ru_issues else '',
            'UA_Issues': '; '.join(ua_issues) if ua_issues else '',
            'Source_Hash': source_hash,
            'Preserved_Tokens': self._extract_preserved_tokens(result)
        }
    
    def _calculate_source_hash(self, result: Dict[str, Any]) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö–µ—à –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è"""
        import hashlib
        content = f"{result.get('url', '')}{result.get('RU_HTML', '')}{result.get('UA_HTML', '')}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()[:8]
    
    def _extract_preserved_tokens(self, result: Dict[str, Any]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        tokens = []
        for locale in ['ru', 'ua']:
            html_key = f'{locale.upper()}_HTML'
            html_content = result.get(html_key, '')
            if html_content:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ HTML
                words = re.findall(r'\b[–∞-—è—ë—ñ—ó—î“ë]+\b', html_content.lower())
                tokens.extend(set(words))
        return ', '.join(sorted(set(tokens))[:10])  # –ü–µ—Ä–≤—ã–µ 10 —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
    
    def _update_specs_metrics(self, result: Dict[str, Any]) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""
        for locale in ['ru', 'ua']:
            html_key = f'{locale.upper()}_HTML'
            html_content = result.get(html_key, '')
            
            if html_content:
                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
                specs_count = self._count_specs_in_html(html_content)
                self.total_specs_processed += specs_count
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç specs_count ‚àà [3,8]
                if not (3 <= specs_count <= 8):
                    logger.warning(f"‚ùå –ù–∞—Ä—É—à–µ–Ω–∏–µ –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–∞ specs_count: {specs_count} –Ω–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [3,8] –¥–ª—è {locale}")
                else:
                    logger.debug(f"‚úÖ –ò–Ω–≤–∞—Ä–∏–∞–Ω—Ç specs_count —Å–æ–±–ª—é–¥–µ–Ω: {specs_count} ‚àà [3,8] –¥–ª—è {locale}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø—Ä–∏ —É—Å–µ—á–µ–Ω–∏–∏ –¥–æ 8
                if specs_count == 8:
                    self._check_specs_priority(html_content, locale)
                
                # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —É—Å–µ—á–µ–Ω–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏ (–µ—Å–ª–∏ –±—ã–ª–æ 8, –≤–µ—Ä–æ—è—Ç–Ω–æ —É—Å–µ—á–µ–Ω–æ)
                if specs_count == 8:
                    self.specs_clamped_count += 1

    def _update_note_buy_metrics(self, result: Dict[str, Any]) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è note-buy"""
        flags = result.get('flags', [])
        
        for flag in flags:
            # –†–∞–∑–±–∏–≤–∞–µ–º sanitization_info –Ω–∞ —á–∞—Å—Ç–∏
            sanitization_parts = flag.split(';') if ';' in flag else [flag]
            
            for part in sanitization_parts:
                part = part.strip()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥–∏ —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏–∏ note-buy
                if 'note_buy_sanitized=true' in part:
                    self.note_buy_sanitized_count += 1
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                if 'note_buy_source=original' in part:
                    self.note_buy_source_stats['original'] += 1
                elif 'note_buy_source=safe_constructor' in part:
                    self.note_buy_source_stats['safe_constructor'] += 1
                elif 'note_buy_source=failed_sanitization' in part:
                    self.note_buy_source_stats['failed_sanitization'] += 1

    def _update_title_metrics(self, result: Dict[str, Any]) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
        flags = result.get('flags', [])
        
        for flag in flags:
            # –†–∞–∑–±–∏–≤–∞–µ–º sanitization_info –Ω–∞ —á–∞—Å—Ç–∏
            sanitization_parts = flag.split(';') if ';' in flag else [flag]
            
            for part in sanitization_parts:
                part = part.strip()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥–∏ —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏–∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                if 'title_sanitized=true' in part:
                    self.title_sanitized_count += 1
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                if 'title_source=original' in part:
                    self.title_source_stats['original'] += 1
                elif 'title_source=safe_constructor' in part:
                    self.title_source_stats['safe_constructor'] += 1
                elif 'title_source=failed_sanitization' in part:
                    self.title_source_stats['failed_sanitization'] += 1
    
    def _count_specs_in_html(self, html_content: str) -> int:
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –≤ HTML"""
        import re
        specs_match = re.search(r'<ul class="specs">(.*?)</ul>', html_content, re.DOTALL)
        if specs_match:
            return len(re.findall(r'<li>', specs_match.group(1)))
        return 0
    
    def _check_specs_priority(self, html_content: str, locale: str) -> None:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –ø—Ä–∏ —É—Å–µ—á–µ–Ω–∏–∏ –¥–æ 8"""
        import re
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        specs_match = re.search(r'<ul class="specs">(.*?)</ul>', html_content, re.DOTALL)
        if not specs_match:
            return
        
        spec_names = []
        li_matches = re.findall(r'<li>.*?<span class="spec-label">([^:]+):</span>', specs_match.group(1))
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–ª—é—á–∏
        priority_keys = [
            '–±—Ä–µ–Ω–¥', 'brand', '—Ç–∏–ø', 'type', '–æ–±—ä–µ–º', '–æ–±\'—î–º', '–≤–µ—Å', '–≤–∞–≥–∞', '–º–∞—Ç–µ—Ä–∏–∞–ª', '–º–∞—Ç–µ—Ä—ñ–∞–ª',
            '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞', '–∑–æ–Ω—ã', '–∑–æ–Ω–∏', '–æ–±–ª–∞—Å—Ç—å', '—Å–µ—Ä–∏—è', '—Å–µ—Ä—ñ—è', '–∫–ª–∞—Å—Å', '–∫–ª–∞—Å', '—Ü–≤–µ—Ç', '–∫–æ–ª—ñ—Ä'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –∫–ª—é—á–µ–π
        found_priorities = []
        for name in li_matches:
            name_lower = name.lower().strip()
            for priority in priority_keys:
                if priority in name_lower:
                    found_priorities.append(name)
                    break
        
        # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ –º–µ–Ω—å—à–µ 4 –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –∫–ª—é—á–µ–π, —Å—á–∏—Ç–∞–µ–º –Ω–∞—Ä—É—à–µ–Ω–∏–µ–º
        if len(found_priorities) < 4:
            self.specs_priority_violations += 1
            logger.warning(f"‚ö†Ô∏è –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –¥–ª—è {locale}: –Ω–∞–π–¥–µ–Ω–æ {len(found_priorities)} –∏–∑ {len(priority_keys)} –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –∫–ª—é—á–µ–π")
    
    def _get_available_filename(self, base_filename: str) -> str:
        """–ù–∞—Ö–æ–¥–∏—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞, –¥–æ–±–∞–≤–ª—è—è –Ω–æ–º–µ—Ä –µ—Å–ª–∏ —Ñ–∞–π–ª –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω"""
        if not os.path.exists(base_filename):
            return base_filename
        
        # –ü—Ä–æ–±—É–µ–º –∑–∞–ø–∏—Å–∞—Ç—å –≤ —Ñ–∞–π–ª, —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
        try:
            with open(base_filename, 'a'):
                pass
            return base_filename
        except (PermissionError, OSError):
            # –§–∞–π–ª –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω - –∏—â–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ–µ –∏–º—è —Å –Ω–æ–º–µ—Ä–æ–º
            pass
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        name, ext = os.path.splitext(base_filename)
        counter = 2
        
        while True:
            new_filename = f"{name}_{counter}{ext}"
            if not os.path.exists(new_filename):
                return new_filename
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –ª–∏ —Ñ–∞–π–ª —Å –Ω–æ–º–µ—Ä–æ–º
            try:
                with open(new_filename, 'a'):
                    pass
                return new_filename
            except (PermissionError, OSError):
                counter += 1
                continue
    
    def write_final_files(self) -> Dict[str, Any]:
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–æ—Ä—è–¥–∫–∞"""
        stats = {
            'valid_results': len(self.ordered_buffer),
            'repair_items': len(self.repair_report),
            'total_processed': len(self.ordered_buffer) + len(self.repair_report),
            'input_count': self.input_count,
            'output_rows': len(self.ordered_buffer),
            'paired_always': self.mode == "paired_always",
            'order_mismatch_count': 0
        }
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª —Å —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        if self.ordered_buffer:
            actual_output_file = self._get_available_filename(self.output_file)
            try:
                # –°–æ–∑–¥–∞–µ–º —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–π DataFrame
                ordered_data = []
                for i in range(1, self.input_count + 1):
                    if i in self.ordered_buffer:
                        row_data = self.ordered_buffer[i].copy()
                        row_data['InputIndex'] = i
                        # –î–æ–±–∞–≤–ª—è–µ–º Export_Mode
                        export_mode = self._determine_export_mode(row_data)
                        row_data['Export_Mode'] = export_mode
                        ordered_data.append(row_data)
                    else:
                        # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –¥–ª—è –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
                        empty_row = {
                            'URL': self.input_urls[i-1] if i <= len(self.input_urls) else f'missing_{i}',
                            'RU_HTML': '',
                            'UA_HTML': '',
                            'RU_Valid': False,
                            'UA_Valid': False,
                            'RU_Issues': 'Missing data',
                            'UA_Issues': 'Missing data',
                            'Source_Hash': '',
                            'Preserved_Tokens': '',
                            'Export_Mode': 'failed',
                            'InputIndex': i
                        }
                        ordered_data.append(empty_row)
                
                df = pd.DataFrame(ordered_data)
                df.to_excel(actual_output_file, index=False)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –ø–æ—Å–ª–µ –∑–∞–ø–∏—Å–∏
                order_mismatch_count = self._verify_order(df)
                stats['order_mismatch_count'] = order_mismatch_count
                
                if order_mismatch_count > 0:
                    logger.critical(f"‚ùå –ù–∞—Ä—É—à–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞: {order_mismatch_count} –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π")
                    # –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
                    self._rewrite_with_correct_order(actual_output_file, df)
                else:
                    logger.info(f"‚úÖ –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª –∑–∞–ø–∏—Å–∞–Ω: {actual_output_file} ({len(ordered_data)} —Å—Ç—Ä–æ–∫)")
                
                stats['main_file'] = actual_output_file
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {e}")
                stats['main_file_error'] = str(e)
        else:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∑–∞–ø–∏—Å–∏")
            stats['main_file'] = None
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º repair_report
        if self.repair_report:
            try:
                repair_file = f"repair_report_{int(datetime.now().timestamp())}.xlsx"
                df_repair = pd.DataFrame(self.repair_report)
                df_repair.to_excel(repair_file, index=False)
                logger.info(f"üìã Repair report –∑–∞–ø–∏—Å–∞–Ω: {repair_file} ({len(self.repair_report)} —Å—Ç—Ä–æ–∫)")
                stats['repair_file'] = repair_file
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ repair_report: {e}")
                stats['repair_file_error'] = str(e)
        else:
            logger.info("‚úÖ –ù–µ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è repair_report")
            stats['repair_file'] = None
        
        return stats
    
    def update_repair_metrics(self, repair_stats: Dict[str, Any]) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Ä–µ–º–æ–Ω—Ç–∞"""
        self.repair_enqueued_count = repair_stats.get('repair_enqueued_count', 0)
        self.repair_completed_count = repair_stats.get('repair_completed_count', 0)
        self.repair_failed_count = repair_stats.get('repair_failed_count', 0)
        self.sanity_fix_applied_count = repair_stats.get('sanity_fix_applied_count', 0)
        self.repair_reason_stats = repair_stats.get('repair_reason_stats', {})
        
        logger.info(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ–º–æ–Ω—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {self.repair_completed_count} –∑–∞–≤–µ—Ä—à–µ–Ω–æ, {self.repair_failed_count} –Ω–µ—É–¥–∞—á–Ω–æ")
    
    def upsert_repair_result(self, input_index: int, repaired_result: Dict[str, Any]) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ —Ä–µ–º–æ–Ω—Ç–∞"""
        if input_index in self.ordered_buffer:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            original_row = self.ordered_buffer[input_index]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ–±–ª–µ–º–Ω—É—é –ª–æ–∫–∞–ª—å
            failing_locale = repaired_result.get('failing_locale')
            if failing_locale == 'ru':
                original_row['RU_HTML'] = repaired_result.get('repaired_html', '')
                original_row['RU_Valid'] = True
                original_row['RU_Issues'] = repaired_result.get('issues', '')
            elif failing_locale == 'ua':
                original_row['UA_HTML'] = repaired_result.get('repaired_html', '')
                original_row['UA_Valid'] = True
                original_row['UA_Issues'] = repaired_result.get('issues', '')
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–º–æ–Ω—Ç–µ
            original_row['Repair_Applied'] = True
            original_row['Repair_Reason'] = repaired_result.get('repair_reason', '')
            
            logger.info(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±–Ω–æ–≤–ª–µ–Ω –ø–æ—Å–ª–µ —Ä–µ–º–æ–Ω—Ç–∞: –∏–Ω–¥–µ–∫—Å {input_index}, –ª–æ–∫–∞–ª—å {failing_locale}")
        else:
            logger.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: –∏–Ω–¥–µ–∫—Å {input_index}")
    
    def _verify_order(self, df: pd.DataFrame) -> int:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø–æ—Ä—è–¥–æ–∫ URL –≤ DataFrame"""
        mismatches = 0
        for i, row in df.iterrows():
            expected_url = self.input_urls[i] if i < len(self.input_urls) else f'missing_{i+1}'
            actual_url = row.get('URL', '')
            if actual_url != expected_url:
                logger.critical(f"‚ùå –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–æ—Ä—è–¥–∫–∞ –≤ —Å—Ç—Ä–æ–∫–µ {i+1}: –æ–∂–∏–¥–∞–ª—Å—è '{expected_url}', –ø–æ–ª—É—á–µ–Ω '{actual_url}'")
                mismatches += 1
        return mismatches
    
    def _rewrite_with_correct_order(self, filename: str, df: pd.DataFrame) -> None:
        """–ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç —Ñ–∞–π–ª –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ"""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫
            correct_data = []
            for i in range(len(self.input_urls)):
                # –ò—â–µ–º —Å—Ç—Ä–æ–∫—É —Å –Ω—É–∂–Ω—ã–º URL
                matching_rows = df[df['URL'] == self.input_urls[i]]
                if not matching_rows.empty:
                    correct_data.append(matching_rows.iloc[0].to_dict())
                else:
                    # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
                    empty_row = {
                        'URL': self.input_urls[i],
                        'RU_HTML': '',
                        'UA_HTML': '',
                        'RU_Valid': False,
                        'UA_Valid': False,
                        'RU_Issues': 'Missing data',
                        'UA_Issues': 'Missing data',
                        'Source_Hash': '',
                        'Preserved_Tokens': '',
                        'InputIndex': i + 1
                    }
                    correct_data.append(empty_row)
            
            # –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª
            correct_df = pd.DataFrame(correct_data)
            correct_df.to_excel(filename, index=False)
            logger.info(f"‚úÖ –§–∞–π–ª –ø–µ—Ä–µ–ø–∏—Å–∞–Ω –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ: {filename}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —ç–∫—Å–ø–æ—Ä—Ç–∞"""
        total_locales = len(self.ordered_buffer) * 2  # RU + UA –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏
        specs_clamped_rate = (self.specs_clamped_count / total_locales * 100) if total_locales > 0 else 0
        note_buy_sanitized_rate = (self.note_buy_sanitized_count / total_locales * 100) if total_locales > 0 else 0
        title_sanitized_rate = (self.title_sanitized_count / total_locales * 100) if total_locales > 0 else 0
        
        return {
            'valid_results': len(self.ordered_buffer),
            'repair_items': len(self.repair_report),
            'total_processed': len(self.ordered_buffer) + len(self.repair_report),
            'input_count': self.input_count,
            'output_rows': len(self.ordered_buffer),
            'paired_always': self.mode == "paired_always",
            'success_rate': len(self.ordered_buffer) / self.input_count if self.input_count > 0 else 0,
            # –ù–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
            'specs_clamped_count': self.specs_clamped_count,
            'specs_clamped_rate': f"{specs_clamped_rate:.1f}%",
            'total_specs_processed': self.total_specs_processed,
            'specs_priority_violations': self.specs_priority_violations,
            'avg_specs_per_locale': self.total_specs_processed / total_locales if total_locales > 0 else 0,
            # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è note-buy
            'note_buy_sanitized_count': self.note_buy_sanitized_count,
            'note_buy_sanitized_rate': f"{note_buy_sanitized_rate:.1f}%",
            'note_buy_source_stats': self.note_buy_source_stats,
            # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
            'title_sanitized_count': self.title_sanitized_count,
            'title_sanitized_rate': f"{title_sanitized_rate:.1f}%",
            'title_source_stats': self.title_source_stats,
            # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å–∏—Å—Ç–µ–º—ã —Ä–µ–º–æ–Ω—Ç–∞
            'repair_enqueued_count': self.repair_enqueued_count,
            'repair_completed_count': self.repair_completed_count,
            'repair_failed_count': self.repair_failed_count,
            'sanity_fix_applied_count': self.sanity_fix_applied_count,
            'repair_reason_stats': self.repair_reason_stats
        }
