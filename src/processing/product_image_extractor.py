"""
–ò–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ç–æ–≤–∞—Ä–æ–≤ —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ URL –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞
"""
import logging
import re
import requests
from typing import Dict, Any, Optional, List
from bs4 import BeautifulSoup

logger = logging.getLogger(__name__)

class ProductImageExtractor:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤"""
    
    def __init__(self):
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –°–ü–ò–°–û–ö –ò–°–ö–õ–Æ–ß–ï–ù–ò–ô –¥–ª—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –ª–æ–≥–æ—Ç–∏–ø–æ–≤
        self.logo_blacklist = [
            '87680824850809',      # –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ª–æ–≥–æ—Ç–∏–ø –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–æ—Ö–æ–¥–∏—Ç!
            '200x44',              # –†–∞–∑–º–µ—Ä—ã –ª–æ–≥–æ—Ç–∏–ø–æ–≤
            '100x50',
            '150x75',
            '50x50',               # –ú–∏–Ω–∏–∞—Ç—é—Ä—ã
            '60x25',
            '88x20',
            'logo',
            'brand',
            'header',
            'footer',
            'nav',
            'menu',
            'icon',
            'watermark',
            'stamp',
            'signature',
            'epilax-89712168840516',  # –ï—â–µ –æ–¥–∏–Ω –ª–æ–≥–æ—Ç–∏–ø
        ]
        
        # –ö–∞—Ä—Ç–∞ —Ç–æ–≤–∞—Ä–æ–≤ –∏ –∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        self.product_image_map = {
            # –ì–µ–ª–∏ –¥–ª—è –¥—É—à–∞
            'gel-dlia-dush': {
                'kokos': 'gel-coconut-250ml.webp',
                'vetiver': 'gel-vetiver-250ml.webp',
                'aqua-blue': 'gel-aqua-blue-250ml.webp',
                'bilyi-chai': 'gel-white-tea-250ml.webp',
                'morska-sil': 'gel-sea-salt-250ml.webp'
            },
            
            # –ì–µ–ª–∏ –¥–æ –¥–µ–ø–∏–ª—è—Ü–∏–∏
            'hel-do-depiliatsii': {
                'okholodzhuiuchym': 'gel-cooling-effect-250ml.webp',
                'default': 'gel-pre-depilation-250ml.webp'
            },
            
            # –ü—É–¥—Ä—ã
            'pudra': {
                'enzymna': 'powder-enzymatic-50g.webp',
                'default': 'powder-epilax-50g.webp'
            },
            
            # –ü–µ–Ω–∫–∏
            'pinka': {
                'intymnoi': 'foam-intimate-150ml.webp',
                'zhyrnoi': 'foam-oily-skin-150ml.webp',
                'sukhoi': 'foam-dry-skin-150ml.webp',
                'default': 'foam-cleansing-150ml.webp'
            },
            
            # –§–ª—é–∏–¥—ã
            'fliuid': {
                'vrosloho-volossia': 'fluid-ingrown-hair-5ml.webp',
                'default': 'fluid-epilax-5ml.webp'
            }
        }

    def is_valid_product_image(self, image_url: str) -> bool:
        """–£–°–ò–õ–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –ª–æ–≥–æ—Ç–∏–ø–æ–≤"""
        
        url_lower = image_url.lower()
        
        # –ñ–ï–°–¢–ö–ê–Ø –ë–õ–û–ö–ò–†–û–í–ö–ê –ª–æ–≥–æ—Ç–∏–ø–æ–≤
        for blacklist_item in self.logo_blacklist:
            if blacklist_item in url_lower:
                logger.debug(f"üö´ –ó–ê–ë–õ–û–ö–ò–†–û–í–ê–ù–û: {image_url} (–Ω–∞–π–¥–µ–Ω: {blacklist_item})")
                return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–∏–Ω–∏–∞—Ç—é—Ä—ã (—Ä–∞–∑–º–µ—Ä—ã –º–µ–Ω—å—à–µ 200px)
        size_patterns = [
            r'/\d+x\d+[a-z0-9]*/',  # –õ—é–±—ã–µ —Ä–∞–∑–º–µ—Ä—ã –≤ URL
        ]
        
        for pattern in size_patterns:
            matches = re.findall(pattern, url_lower)
            for match in matches:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã
                size_match = re.search(r'/(\d+)x(\d+)', match)
                if size_match:
                    width, height = int(size_match.group(1)), int(size_match.group(2))
                    if width < 200 or height < 200:
                        logger.debug(f"üö´ –ó–ê–ë–õ–û–ö–ò–†–û–í–ê–ù–û (–º–∏–Ω–∏–∞—Ç—é—Ä–∞): {image_url} ({width}x{height})")
                        return False
        
        # –¢–û–õ–¨–ö–û –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤
        quality_indicators = [
            'content/images',
            '.webp',
            '.jpg',
            '.png'
        ]
        
        has_quality = any(indicator in url_lower for indicator in quality_indicators)
        
        if has_quality:
            logger.info(f"‚úÖ –ö–ê–ß–ï–°–¢–í–ï–ù–ù–û–ï –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï: {image_url}")
            return True
        
        logger.warning(f"‚ùå –ù–ï –ü–†–û–®–õ–û –ü–†–û–í–ï–†–ö–£: {image_url}")
        return False

    def verify_image_exists(self, image_url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ URL"""
        
        try:
            response = requests.head(image_url, timeout=5)
            
            if response.status_code == 200:
                content_type = response.headers.get('content-type', '')
                if 'image' in content_type:
                    logger.info(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ: {image_url}")
                    return True
                    
            logger.warning(f"‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ: {image_url} (—Å—Ç–∞—Ç—É—Å: {response.status_code})")
            return False
            
        except Exception as e:
            logger.warning(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_url}: {e}")
            return False

    def extract_real_product_image_from_html(self, html_content: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –†–ï–ê–õ–¨–ù–´–ô URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–∞ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –≤—ã—Å–æ–∫–∏–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º"""

        soup = BeautifulSoup(html_content, 'html.parser')

        image_selectors = [
            'img[src*="/content/images/"]',
            '.product-gallery img',
            '.product__gallery img',
            '.product-image img',
            '.product-photo img',
            'img[alt*="Epilax"]',
            'img[src*=".webp"]',
            'img[src*=".jpg"]',
            'img[src*=".png"]'
        ]

        found_images = []

        for selector in image_selectors:
            try:
                img_elements = soup.select(selector)
                for img in img_elements:
                    src = img.get('src')
                    if not src:
                        continue

                    if src.startswith('/'):
                        src = f"https://prorazko.com{src}"

                    if not src.startswith('http'):
                        src = f"https://prorazko.com/{src.lstrip('/')}"

                    lowered = src.lower()
                    if any(keyword in lowered for keyword in ['content/images', 'epilax', 'product']):
                        # –ü–†–û–í–ï–†–Ø–ï–ú –í–ê–õ–ò–î–ù–û–°–¢–¨ –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø
                        if self.is_valid_product_image(src):
                            found_images.append(src)
                            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω –í–ê–õ–ò–î–ù–´–ô URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {src}")
                        else:
                            logger.warning(f"üö´ –ù–ï–í–ê–õ–ò–î–ù–û–ï –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {src}")
            except Exception as selector_error:
                logger.debug(f"‚ùå –°–µ–ª–µ–∫—Ç–æ—Ä {selector} –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {selector_error}")
                continue

        if not found_images:
            logger.warning("‚ö†Ô∏è –†–µ–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ HTML")
            return None

        # –¢–µ–ø–µ—Ä—å –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é
        best_image = self._get_high_quality_image(found_images)
        return best_image

    def extract_tmgallery_images_from_js(self, html_content: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ TMGallery JavaScript"""
        
        # –ò—â–µ–º TMGallery JavaScript –∫–æ–¥
        js_patterns = [
            r'tmGallery.*?images.*?\[(.*?)\]',
            r'gallery.*?images.*?\[(.*?)\]',
            r'productImages.*?\[(.*?)\]',
        ]
        
        for pattern in js_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE | re.DOTALL)
            for match in matches:
                # –ò—â–µ–º URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –Ω–∞–π–¥–µ–Ω–Ω–æ–º –∫–æ–¥–µ
                image_urls = re.findall(r'["\']([^"\']*content/images[^"\']*)["\']', match)
                
                for url in image_urls:
                    if self.is_valid_product_image(url):
                        logger.info(f"‚úÖ TMGallery –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {url}")
                        return url
        
        return None

    def extract_main_product_image_from_html(self, html_content: str) -> Optional[str]:
        """–ò—â–µ—Ç –û–°–ù–û–í–ù–û–ï –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –Ω–∞ –≥–∞–ª–µ—Ä–µ—é"""
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –ü–†–ò–û–†–ò–¢–ï–¢–ù–´–ï —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ç–æ–≤–∞—Ä–æ–≤
        priority_selectors = [
            '.tmGallery-image img',           # –û—Å–Ω–æ–≤–Ω–∞—è –≥–∞–ª–µ—Ä–µ—è
            '.tmGallery-main img',            # –ì–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥–∞–ª–µ—Ä–µ–∏
            '.product__gallery img',          # –ì–∞–ª–µ—Ä–µ—è –ø—Ä–æ–¥—É–∫—Ç–∞
            '.product-image img',             # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–æ–¥—É–∫—Ç–∞
            '.gallery-item img',              # –≠–ª–µ–º–µ–Ω—Ç—ã –≥–∞–ª–µ—Ä–µ–∏
            '.product-photo img',             # –§–æ—Ç–æ –ø—Ä–æ–¥—É–∫—Ç–∞
            '.main-image img',                # –ì–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        ]
        
        for selector in priority_selectors:
            try:
                images = soup.select(selector)
                logger.info(f"üîç –°–µ–ª–µ–∫—Ç–æ—Ä '{selector}' –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")
                
                for img in images:
                    src = img.get('src')
                    if src:
                        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π URL
                        if src.startswith('/'):
                            src = f"https://prorazko.com{src}"
                        
                        logger.info(f"üì∏ –ö–∞–Ω–¥–∏–¥–∞—Ç: {src}")
                        
                        # –ñ–ï–°–¢–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê
                        if self.is_valid_product_image(src):
                            logger.info(f"üéØ –í–´–ë–†–ê–ù–û –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï: {src}")
                            return src
                        
            except Exception as e:
                logger.debug(f"‚ùå –û—à–∏–±–∫–∞ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞ '{selector}': {e}")
        
        logger.warning(f"üö´ –ù–ï –ù–ê–ô–î–ï–ù–û –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        return None

    def extract_gallery_images_by_priority(self, html_content: str) -> Optional[str]:
        """–†–µ–∑–µ—Ä–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –ù–∞–π—Ç–∏ –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ content/images
        all_images = soup.find_all('img', src=True)
        
        candidates = []
        
        for img in all_images:
            src = img.get('src')
            
            if src and 'content/images' in src:
                # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π URL
                if src.startswith('/'):
                    src = f"https://prorazko.com{src}"
                
                # –ñ–ï–°–¢–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
                if self.is_valid_product_image(src):
                    
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ —Ä–∞–∑–º–µ—Ä—É –≤ URL (–æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã)
                    priority_score = 0
                    
                    if '2048x2048' in src:
                        priority_score = 150  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                    elif '1800x1800' in src:
                        priority_score = 140  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                    elif '1600x1600' in src:
                        priority_score = 130  # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                    elif '1280x1280' in src:
                        priority_score = 120  # –•–æ—Ä–æ—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                    elif '1024x1024' in src:
                        priority_score = 110  # –•–æ—Ä–æ—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                    elif '800x800' in src:
                        priority_score = 100  # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                    elif '600x600' in src:
                        priority_score = 90   # –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                    elif '.webp' in src:
                        priority_score = 50   # –§–æ—Ä–º–∞—Ç WebP
                    elif '.jpg' in src or '.png' in src:
                        priority_score = 30   # –ë–∞–∑–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
                    
                    candidates.append((src, priority_score))
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        candidates.sort(key=lambda x: x[1], reverse=True)
        
        if candidates:
            best_image = candidates[0][0]
            logger.info(f"üèÜ –õ–£–ß–®–ï–ï –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï: {best_image} (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {candidates[0][1]})")
            return best_image
        
        return None

    def _get_high_quality_image(self, image_urls: List[str]) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞, –ø—Ä–æ–≤–µ—Ä—è—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è"""
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–∞–∫ –±–∞–∑–æ–≤–æ–µ
        base_url = image_urls[0]
        logger.info(f"üîç –ò—â–µ–º –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –¥–ª—è: {base_url}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_variants = self._generate_quality_variants(base_url)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞
        for variant_url in quality_variants:
            if self._check_image_availability(variant_url):
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {variant_url}")
                return variant_url
        
        # –ï—Å–ª–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π
        logger.warning(f"‚ö†Ô∏è –í—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º: {base_url}")
        return base_url

    def _generate_quality_variants(self, base_url: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã URL –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        
        variants = []
        
        # –ï—Å–ª–∏ URL —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞–∑–º–µ—Ä–Ω—É—é –º–µ—Ç–∫—É (–Ω–∞–ø—Ä–∏–º–µ—Ä, /200x44l90nn0/), –∑–∞–º–µ–Ω—è–µ–º –µ—ë
        if re.search(r'/\d+x\d+[a-z0-9]*/', base_url):
            # –£–¥–∞–ª—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω—É—é –º–µ—Ç–∫—É –ø–æ–ª–Ω–æ—Å—Ç—å—é
            no_size_url = re.sub(r'/\d+x\d+[a-z0-9]*/', '/', base_url)
            variants.append(no_size_url)
            
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –≤—ã—Å–æ–∫–∏–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞)
            for size in ['2048x2048', '1800x1800', '1600x1600', '1280x1280', '1024x1024', 'original']:
                high_quality_url = re.sub(r'/\d+x\d+[a-z0-9]*/', f'/{size}/', base_url)
                variants.append(high_quality_url)
        
        # –¢–∞–∫–∂–µ –ø–æ–ø—Ä–æ–±—É–µ–º –¥–æ–±–∞–≤–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω—ã–µ –º–µ—Ç–∫–∏ –∫ URL –±–µ–∑ –Ω–∏—Ö
        if not re.search(r'/\d+x\d+[a-z0-9]*/', base_url):
            for size in ['2048x2048', '1800x1800', '1600x1600', '1280x1280', '1024x1024', '800x800']:
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é –ø–µ—Ä–µ–¥ –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞
                if '/content/images/' in base_url:
                    parts = base_url.split('/content/images/')
                    if len(parts) == 2:
                        high_quality_url = f"{parts[0]}/content/images/{size}/{parts[1]}"
                        variants.append(high_quality_url)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        unique_variants = []
        for variant in variants:
            if variant not in unique_variants:
                unique_variants.append(variant)
        
        logger.info(f"üîç –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(unique_variants)} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞")
        return unique_variants

    def _check_image_availability(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ URL"""
        try:
            response = requests.head(url, timeout=5, allow_redirects=True)
            if response.status_code == 200:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                content_type = response.headers.get('content-type', '').lower()
                if any(img_type in content_type for img_type in ['image/', 'jpeg', 'jpg', 'png', 'webp']):
                    logger.debug(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω–æ: {url} (Content-Type: {content_type})")
                    return True
                else:
                    logger.debug(f"‚ùå URL –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º: {url} (Content-Type: {content_type})")
                    return False
            else:
                logger.debug(f"‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ: {url} (Status: {response.status_code})")
                return False
        except Exception as e:
            logger.debug(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {url}: {e}")
            return False

    def generate_fallback_image_url(self, product_url: str) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ–µ –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—É"""

        fallback_map = {
            'hel-dlia-dushu': 'https://prorazko.com/content/images/47180_1.webp',
            'pudra-enzymna': 'https://prorazko.com/content/images/47181_1.webp',
            'fliuid-vid-vrosloho': 'https://prorazko.com/content/images/47182_1.webp'
        }

        for slug_part, fallback_url in fallback_map.items():
            if slug_part in product_url:
                logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è {slug_part}: {fallback_url}")
                return fallback_url

        # –í–º–µ—Å—Ç–æ –¥–µ—Ñ–æ–ª—Ç–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
        logger.warning(f"‚ö†Ô∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –¥–ª—è —Ç–æ–≤–∞—Ä–∞: {product_url}")
        return None
    
    def generate_product_image_url(self, product_url: str, product_title: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —Ç–æ–≤–∞—Ä–∞"""
        
        logger.info(f"üîç –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è: {product_url}")
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Ç–æ–≤–∞—Ä–∞
        for product_type, variations in self.product_image_map.items():
            if product_type in product_url:
                # –ù–∞–π—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
                for variant_key, image_file in variations.items():
                    if variant_key == 'default':
                        continue
                    if variant_key in product_url:
                        image_url = f"https://prorazko.com/content/images/{image_file}"
                        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: {variant_key} -> {image_url}")
                        return image_url
                
                # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å default –≤–∞—Ä–∏–∞–Ω—Ç
                if 'default' in variations:
                    image_url = f"https://prorazko.com/content/images/{variations['default']}"
                    logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º default –≤–∞—Ä–∏–∞–Ω—Ç: {image_url}")
                    return image_url
        
        # Fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        fallback_url = "https://prorazko.com/content/images/epilax-product-default.webp"
        logger.warning(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {fallback_url}")
        return fallback_url
    
    def create_product_image_alt(self, product_title: str, locale: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π ALT-—Ç–µ–≥ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–∞"""
        
        if locale == 'ua':
            return f'{product_title} ‚Äî –∫—É–ø–∏—Ç–∏ –∑ –¥–æ—Å—Ç–∞–≤–∫–æ—é –ø–æ –£–∫—Ä–∞—ó–Ω—ñ –≤ –º–∞–≥–∞–∑–∏–Ω—ñ ProRazko'
        else:  # ru
            return f'{product_title} ‚Äî –∫—É–ø–∏—Ç—å —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π –ø–æ –£–∫—Ä–∞–∏–Ω–µ –≤ –º–∞–≥–∞–∑–∏–Ω–µ ProRazko'
    
    def get_product_image_data(self, html_content: str, product_url: str, product_title: str, locale: str) -> Dict[str, str]:
        """–û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –º–µ—Ç–æ–¥ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        
        logger.info(f"\n{'='*80}")
        logger.info(f"üéØ –ü–û–ò–°–ö –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø –î–õ–Ø: {product_title}")
        logger.info(f"üåê URL: {product_url}")
        logger.info(f"{'='*80}")
        
        soup = BeautifulSoup(html_content, 'lxml')
        image_url = None
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ò—â–µ–º –°–†–ê–ó–£ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è 600x600, 800x800 –∏ —Ç.–¥.
        logger.info("üîç –≠–¢–ê–ü 1: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        quality_patterns = ['600x600', '800x800', '1000x1000', '500x500', '1024x1024', '400x400', '300x300']
        
        for pattern in quality_patterns:
            # –ò—â–µ–º –≤ HTML –Ω–∞–ø—Ä—è–º—É—é (–±—ã—Å—Ç—Ä–µ–µ —á–µ–º –ø–µ—Ä–µ–±–æ—Ä)
            quality_images = soup.find_all('img', src=re.compile(f'{pattern}'))
            
            if quality_images:
                for img in quality_images:
                    src = img.get('src')
                    if src and self.is_valid_product_image(src):
                        image_url = self._ensure_absolute_url(src)
                        logger.info(f"‚úÖ –≠–¢–ê–ü 1: –ù–∞–π–¥–µ–Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {pattern}: {image_url}")
                        break
                if image_url:
                    break
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 2: –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ data-–∞—Ç—Ä–∏–±—É—Ç—ã –∏ lazy loading
        if not image_url:
            logger.info("üîç –≠–¢–ê–ü 2: –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ data-–∞—Ç—Ä–∏–±—É—Ç—ã (data-src, data-lazy-src)...")
            data_attrs = ['data-src', 'data-lazy-src', 'data-original', 'data-image']
            
            for attr in data_attrs:
                images_with_data = soup.find_all('img', attrs={attr: True})
                for img in images_with_data:
                    src = img.get(attr)
                    if src and '/content/images/' in src and self.is_valid_product_image(src):
                        image_url = self._ensure_absolute_url(src)
                        logger.info(f"‚úÖ –≠–¢–ê–ü 2: –ù–∞–π–¥–µ–Ω–æ —á–µ—Ä–µ–∑ {attr}: {image_url}")
                        break
                if image_url:
                    break
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 3: –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
        if not image_url:
            logger.info("üîç –≠–¢–ê–ü 3: –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã...")
            priority_selectors = [
                'img[src*="600x600"]',
                'img[src*="content/images"]',
                '.product-gallery img',
                '.tmGallery-image img',
                '.tmGallery-main img',
                '.product-photo img',
                '.product-image img',
                '#product-image img'
            ]
            
            for selector in priority_selectors:
                imgs = soup.select(selector)
                for img in imgs:
                    src = img.get('src')
                    if src and not self._is_thumbnail(src) and self.is_valid_product_image(src):
                        image_url = self._ensure_absolute_url(src)
                        logger.info(f"‚úÖ –≠–¢–ê–ü 3: –ù–∞–π–¥–µ–Ω–æ —á–µ—Ä–µ–∑ —Å–µ–ª–µ–∫—Ç–æ—Ä {selector}: {image_url}")
                        break
                if image_url:
                    break
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 4: –ê–ì–†–ï–°–°–ò–í–ù–´–ô –ø–æ–∏—Å–∫ –õ–Æ–ë–´–• –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ /content/images/
        if not image_url:
            logger.info("üîç –≠–¢–ê–ü 4: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –í–°–ï–• –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ /content/images/...")
            all_images = soup.find_all('img', src=True)
            
            candidates = []
            for img in all_images:
                src = img.get('src', '')
                # –ò—â–µ–º –õ–Æ–ë–´–ï –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ /content/images/
                if src and '/content/images/' in src:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –º–∏–Ω–∏–∞—Ç—é—Ä—ã –∏ –ª–æ–≥–æ—Ç–∏–ø—ã
                    if not self._is_thumbnail(src) and self.is_valid_product_image(src):
                        absolute_url = self._ensure_absolute_url(src)
                        # –î–∞—ë–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º —Å —Ä–∞–∑–º–µ—Ä–∞–º–∏
                        priority = 100 if any(p in src for p in ['600x600', '800x800', '400x400']) else 50
                        candidates.append((absolute_url, priority))
                        logger.info(f"   üì∏ –ö–∞–Ω–¥–∏–¥–∞—Ç: {absolute_url} (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {priority})")
            
            if candidates:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ –±–µ—Ä—ë–º –ª—É—á—à–µ–µ
                candidates.sort(key=lambda x: x[1], reverse=True)
                image_url = candidates[0][0]
                logger.info(f"‚úÖ –≠–¢–ê–ü 4: –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç {candidates[0][1]}): {image_url}")
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 5: TMGallery JavaScript (–µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ)
        if not image_url:
            logger.info("üîç –≠–¢–ê–ü 5: TMGallery JavaScript...")
            image_url = self.extract_tmgallery_images_from_js(html_content)
            if image_url:
                image_url = self._ensure_absolute_url(image_url)
                logger.info(f"‚úÖ TMGallery –£–°–ü–ï–•: {image_url}")
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 6: –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback (—Å–ø–µ—Ü–µ—Ñ–∏—á–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã —Å –º–∞–ø–ø–∏–Ω–≥–æ–º)
        if not image_url:
            logger.info("üîç –≠–¢–ê–ü 6: –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã)...")
            image_url = self.generate_fallback_image_url(product_url)
            if image_url:
                logger.info(f"‚úÖ –≠–¢–ê–ü 6: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π fallback: {image_url}")
            else:
                logger.warning(f"‚ö†Ô∏è –≠–¢–ê–ü 6: Fallback —Ç–æ–∂–µ –Ω–µ –≤–µ—Ä–Ω—É–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")

        # –°–æ–∑–¥–∞—Ç—å ALT-—Ç–µ–≥
        alt_text = self.create_product_image_alt(product_title, locale)
        
        # –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê
        if image_url:
            logger.info(f"üéâ –§–ò–ù–ê–õ–¨–ù–û–ï –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï: {image_url}")
        else:
            logger.warning(f"üö´ –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï –ù–ï –ù–ê–ô–î–ï–ù–û - —Ç–æ–≤–∞—Ä –±—É–¥–µ—Ç –ë–ï–ó –§–û–¢–û")
            image_url = None
        
        result = {
            'url': image_url,
            'alt': alt_text
        }
        
        logger.info(f"üìã –†–ï–ó–£–õ–¨–¢–ê–¢: {result}")
        logger.info(f"{'='*80}\n")
        
        return result
    
    def _is_thumbnail(self, src: str) -> bool:
        """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–∞—Ç—é—Ä"""
        thumbnail_patterns = [
            '50x50', '60x60', '78x78', '80x80', '100x100',
            '40x', '50x33', '50x25', '50x19', '60x25', '88x20'
        ]
        src_lower = src.lower()
        return any(pattern in src_lower for pattern in thumbnail_patterns)
    
    def _ensure_absolute_url(self, url: str) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π URL –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π"""
        if url.startswith('/'):
            return f"https://prorazko.com{url}"
        elif not url.startswith('http'):
            return f"https://prorazko.com/{url.lstrip('/')}"
        return url
