"""
Sanity-—Ñ–∏–∫—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–ø–∏—Å—ã–≤–∞–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏–π
"""
import logging
import re
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

class SanityFixer:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ñ–∏–∫—Å –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –ø—Ä–æ—Ö–æ–¥—è—Ç –≤–∞–ª–∏–¥–∞—Ü–∏—é"""
    
    def __init__(self):
        self.safe_sentences = {
            'ru': [
                "–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –∫–æ–∂–∏ –∏ –≤–æ–ª–æ—Å.",
                "–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –≤–æ–ª–æ—Å.",
                "–ò–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –¥–æ–º–∞—à–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.",
                "–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø–ª–∞–≤–ª–µ–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.",
                "–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–π –∫–æ–∂–∏.",
                "–û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –¥–ª–∏—Ç–µ–ª—å–Ω—É—é –≥–ª–∞–¥–∫–æ—Å—Ç—å –∫–æ–∂–∏.",
                "–ü—Ä–æ—Å—Ç–æ—Ç–∞ –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∏ –±—ã—Å—Ç—Ä—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.",
                "–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–æ–Ω —Ç–µ–ª–∞."
            ],
            'ua': [
                "–ü—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –≤—Å—ñ—Ö —Ç–∏–ø—ñ–≤ —à–∫—ñ—Ä–∏ —Ç–∞ –≤–æ–ª–æ—Å—Å—è.",
                "–ó–∞–±–µ–∑–ø–µ—á—É—î –∫–æ–º—Ñ–æ—Ä—Ç–Ω–µ —Ç–∞ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–µ –≤–∏–¥–∞–ª–µ–Ω–Ω—è –≤–æ–ª–æ—Å—Å—è.",
                "–Ü–¥–µ–∞–ª—å–Ω–æ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –¥–æ–º–∞—à–Ω—å–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è.",
                "–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø–ª–∞–≤–ª–µ–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞ –¥–ª—è –±–µ–∑–ø–µ—á–Ω–æ–≥–æ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è.",
                "–ü—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è —á—É—Ç–ª–∏–≤–æ—ó —à–∫—ñ—Ä–∏.",
                "–ó–∞–±–µ–∑–ø–µ—á—É—î —Ç—Ä–∏–≤–∞–ª—É –≥–ª–∞–¥–∫—ñ—Å—Ç—å —à–∫—ñ—Ä–∏.",
                "–ü—Ä–æ—Å—Ç–æ—Ç–∞ —É –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—ñ —Ç–∞ —à–≤–∏–¥–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.",
                "–ü—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è —Ä—ñ–∑–Ω–∏—Ö –∑–æ–Ω —Ç—ñ–ª–∞."
            ]
        }
    
    def fix_description(self, description: str, locale: str, specs: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç sanity-—Ñ–∏–∫—Å –∫ –æ–ø–∏—Å–∞–Ω–∏—é
        
        Args:
            description: –¢–µ–∫—É—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
            locale: –õ–æ–∫–∞–ª—å ('ru' –∏–ª–∏ 'ua')
            specs: –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ —Ç–æ–≤–∞—Ä–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —Ñ–∏–∫—Å–∞
        """
        if not description or not description.strip():
            logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è —Ñ–∏–∫—Å–∞ (locale: {locale})")
            return {
                'success': False,
                'reason': 'empty_description',
                'fixed_description': description
            }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—É—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
        original_sentences = self._count_sentences(description)
        original_length = len(description.strip())
        
        logger.info(f"üîç –ê–Ω–∞–ª–∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è {locale}: {original_sentences} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, {original_length} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        sentences_needed = max(0, 4 - original_sentences)
        chars_needed = max(0, 400 - original_length)
        
        if sentences_needed == 0 and chars_needed <= 50:  # –£–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ
            logger.info(f"‚úÖ –û–ø–∏—Å–∞–Ω–∏–µ —É–∂–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –¥–ª—è {locale}")
            return {
                'success': True,
                'reason': 'already_valid',
                'fixed_description': description,
                'sentences_added': 0,
                'chars_added': 0
            }
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        selected_sentences = self._select_sentences(locale, sentences_needed, specs)
        
        if not selected_sentences:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–±—Ä–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è —Ñ–∏–∫—Å–∞ (locale: {locale})")
            return {
                'success': False,
                'reason': 'no_sentences_available',
                'fixed_description': description
            }
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–∫—Å
        fixed_description = self._apply_fix(description, selected_sentences)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        new_sentences = self._count_sentences(fixed_description)
        new_length = len(fixed_description.strip())
        
        logger.info(f"‚úÖ Sanity-—Ñ–∏–∫—Å –ø—Ä–∏–º–µ–Ω–µ–Ω –¥–ª—è {locale}: –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(selected_sentences)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, {new_length - original_length} —Å–∏–º–≤–æ–ª–æ–≤")
        
        return {
            'success': True,
            'reason': 'fix_applied',
            'fixed_description': fixed_description,
            'sentences_added': len(selected_sentences),
            'chars_added': new_length - original_length,
            'original_sentences': original_sentences,
            'new_sentences': new_sentences,
            'original_length': original_length,
            'new_length': new_length
        }
    
    def _count_sentences(self, text: str) -> int:
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ —Ç–µ–∫—Å—Ç–µ"""
        if not text or not text.strip():
            return 0
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∑–Ω–∞–∫–∞–º –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        sentences = re.split(r'[.!?]+', text.strip())
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return len(sentences)
    
    def _select_sentences(self, locale: str, count: int, specs: List[Dict[str, str]] = None) -> List[str]:
        """–í—ã–±–∏—Ä–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –¥–æ–ø–∏—Å—ã–≤–∞–Ω–∏—è"""
        if locale not in self.safe_sentences:
            logger.error(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –ª–æ–∫–∞–ª—å: {locale}")
            return []
        
        available_sentences = self.safe_sentences[locale].copy()
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –º–æ–∂–µ–º –≤—ã–±—Ä–∞—Ç—å –±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        if specs:
            relevant_sentences = self._get_relevant_sentences(locale, specs)
            available_sentences = relevant_sentences + available_sentences
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã, —Å–æ—Ö—Ä–∞–Ω—è—è –ø–æ—Ä—è–¥–æ–∫
        seen = set()
        unique_sentences = []
        for sentence in available_sentences:
            if sentence not in seen:
                seen.add(sentence)
                unique_sentences.append(sentence)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        return unique_sentences[:count] if count > 0 else []
    
    def _get_relevant_sentences(self, locale: str, specs: List[Dict[str, str]]) -> List[str]:
        """–í—ã–±–∏—Ä–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""
        relevant = []
        
        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        specs_dict = {}
        for spec in specs:
            key = spec.get('name', '').lower()
            value = spec.get('value', '').lower()
            specs_dict[key] = value
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø –∫–æ–∂–∏
        if any('–∫–æ–∂–∞' in key or '—à–∫—ñ—Ä' in key for key in specs_dict.keys()):
            if locale == 'ru':
                relevant.append("–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –∫–æ–∂–∏.")
            else:
                relevant.append("–ü—ñ–¥—Ö–æ–¥–∏—Ç—å –¥–ª—è –≤—Å—ñ—Ö —Ç–∏–ø—ñ–≤ —à–∫—ñ—Ä–∏.")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É
        if any('—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä' in key or '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä' in key for key in specs_dict.keys()):
            if locale == 'ru':
                relevant.append("–û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.")
            else:
                relevant.append("–û–ø—Ç–∏–º–∞–ª—å–Ω–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ–≥–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è.")
        
        return relevant
    
    def _apply_fix(self, original_description: str, new_sentences: List[str]) -> str:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ–∏–∫—Å –∫ –æ–ø–∏—Å–∞–Ω–∏—é"""
        if not new_sentences:
            return original_description
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ
        clean_description = original_description.strip()
        if clean_description.endswith('.'):
            clean_description = clean_description[:-1]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        fixed_description = clean_description + '. ' + '. '.join(new_sentences) + '.'
        
        return fixed_description
    
    def validate_fixed_description(self, description: str, min_sentences: int = 4, min_chars: int = 400) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º"""
        sentences = self._count_sentences(description)
        length = len(description.strip())
        
        return sentences >= min_sentences and length >= min_chars

    def apply_service_product_fix(self, llm_content: Dict[str, Any], locale: str, product_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ü—Ä–∏–º–µ–Ω—è–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ñ–∏–∫—Å –¥–ª—è —Å–µ—Ä–≤–∏—Å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ (–æ—á–∏—Å—Ç–∏—Ç–µ–ª–∏/–æ–±–µ–∑–∂–∏—Ä–∏–≤–∞—Ç–µ–ª–∏).
        –°–æ–∑–¥–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π RU –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ FAQ –±–ª–æ–∫ –¥–ª—è —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ —Ç–∏–ø–∞.
        """
        logger.info(f"üîß –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ —Ñ–∏–∫—Å–∞ –¥–ª—è {locale}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç–æ–≤–∞—Ä–∞ –ø–æ URL –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—é
        url = product_data.get('url', '')
        title = product_data.get('title', '')
        
        is_cleaner = any(word in url.lower() for word in ['ochysnyk', 'ochistitel', 'cleaner']) or \
                     any(word in title.lower() for word in ['–æ—á–∏—Å—Ç–∏—Ç–µ–ª—å', '–æ—á–∏—Å–Ω–∏–∫', 'cleaner'])
        
        if not is_cleaner:
            logger.info(f"‚ö†Ô∏è –¢–æ–≤–∞—Ä –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –æ—á–∏—Å—Ç–∏—Ç–µ–ª–µ–º, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å–Ω—ã–π —Ñ–∏–∫—Å –¥–ª—è {locale}")
            return llm_content
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –±—Ä–µ–Ω–¥ –∏ –æ–±—ä–µ–º –∏–∑ specs
        specs = llm_content.get('specs', [])
        brand = "ItalWAX"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        volume = "500 –º–ª"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        for spec in specs:
            if isinstance(spec, dict):
                name = spec.get('name', '').lower()
                value = spec.get('value', '')
                if '–±—Ä–µ–Ω–¥' in name or 'brand' in name:
                    brand = value
                elif '–æ–±—ä–µ–º' in name or 'volume' in name or '–º–ª' in value:
                    volume = value
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è RU
        if locale == 'ru':
            new_title = f"–û—á–∏—Å—Ç–∏—Ç–µ–ª—å –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è –∏ –≤–æ—Å–∫–æ–ø–ª–∞–≤–∞ {brand}, {volume}"
            logger.info(f"üîß –°–æ–∑–¥–∞–Ω RU –∑–∞–≥–æ–ª–æ–≤–æ–∫: {new_title}")
            llm_content['title'] = new_title
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π note_buy –¥–ª—è —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
            llm_content['note_buy'] = f"–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É–¥–∞–ª—è–µ—Ç –æ—Å—Ç–∞—Ç–∫–∏ –≤–æ—Å–∫–∞ –∏ –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è —Å –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è. –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –≤–æ—Å–∫–æ–≤ –∏ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π."
            
            # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π FAQ –¥–ª—è —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ —Ç–∏–ø–∞
            if 'faq' not in llm_content or not llm_content['faq']:
                llm_content['faq'] = [
                    {
                        "question": f"–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å {brand}?",
                        "answer": "–ù–∞–Ω–µ—Å–∏—Ç–µ —Å—Ä–µ–¥—Å—Ç–≤–æ –Ω–∞ –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–Ω—É—é –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å, –æ—Å—Ç–∞–≤—å—Ç–µ –Ω–∞ 2-3 –º–∏–Ω—É—Ç—ã, –∑–∞—Ç–µ–º —É–¥–∞–ª–∏—Ç–µ –æ—Å—Ç–∞—Ç–∫–∏ —Å–∞–ª—Ñ–µ—Ç–∫–æ–π –∏–ª–∏ –≥—É–±–∫–æ–π."
                    },
                    {
                        "question": f"–ü–æ–¥—Ö–æ–¥–∏—Ç –ª–∏ {brand} –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –≤–æ—Å–∫–æ–≤?",
                        "answer": "–î–∞, –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —É–¥–∞–ª—è–µ—Ç –æ—Å—Ç–∞—Ç–∫–∏ –ª—é–±—ã—Ö —Ç–∏–ø–æ–≤ –≤–æ—Å–∫–æ–≤ –∏ –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–π —Å –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è."
                    },
                    {
                        "question": f"–ú–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å {brand} –Ω–∞ –∫–æ–∂–µ?",
                        "answer": "–ù–µ—Ç, —ç—Ç–æ —Å—Ä–µ–¥—Å—Ç–≤–æ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–æ —Ç–æ–ª—å–∫–æ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è. –ò–∑–±–µ–≥–∞–π—Ç–µ –∫–æ–Ω—Ç–∞–∫—Ç–∞ —Å –∫–æ–∂–µ–π."
                    },
                    {
                        "question": f"–ö–∞–∫ —á–∞—Å—Ç–æ –Ω—É–∂–Ω–æ –æ—á–∏—â–∞—Ç—å –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ {brand}?",
                        "answer": "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ—á–∏—â–∞—Ç—å –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è –≥–∏–≥–∏–µ–Ω—ã –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏."
                    },
                    {
                        "question": f"–ë–µ–∑–æ–ø–∞—Å–µ–Ω –ª–∏ {brand} –¥–ª—è –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã—Ö –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π?",
                        "answer": "–î–∞, –æ—á–∏—Å—Ç–∏—Ç–µ–ª—å –±–µ–∑–æ–ø–∞—Å–µ–Ω –¥–ª—è –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã—Ö –∏ –º–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏—Ö –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–µ–π –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è."
                    },
                    {
                        "question": f"–ù—É–∂–Ω–æ –ª–∏ —Å–º—ã–≤–∞—Ç—å {brand} –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è?",
                        "answer": "–î–∞, –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ—Ç–µ—Ä–µ—Ç—å –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å –≤–ª–∞–∂–Ω–æ–π —Å–∞–ª—Ñ–µ—Ç–∫–æ–π –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —É–¥–∞–ª–µ–Ω–∏—è –æ—Å—Ç–∞—Ç–∫–æ–≤ —Å—Ä–µ–¥—Å—Ç–≤–∞."
                    }
                ]
                logger.info(f"üîß –°–æ–∑–¥–∞–Ω FAQ –±–ª–æ–∫ –¥–ª—è —Å–µ—Ä–≤–∏—Å–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞: {len(llm_content['faq'])} Q&A")
        
        return llm_content

    def localize_specs_keys_with_llm(self, specs: List[Dict[str, str]], locale: str) -> Dict[str, Any]:
        """
        –õ–æ–∫–∞–ª–∏–∑—É–µ—Ç –∫–ª—é—á–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ —á–µ—Ä–µ–∑ LLM —Å –∂–µ—Å—Ç–∫–∏–º whitelist.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–º–∏ specs –∏ —Ñ–ª–∞–≥–æ–º —É—Å–ø–µ—Ö–∞.
        """
        logger.info(f"üîß LLM-–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª—é—á–µ–π –¥–ª—è {locale}")
        
        # –ë–µ–ª—ã–µ —Å–ø–∏—Å–∫–∏ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –∫–ª—é—á–µ–π
        whitelist = {
            'ru': [
                '–¢–∏–ø —Å—Ä–µ–¥—Å—Ç–≤–∞', '–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ', '–û–±—ä—ë–º', '–í–µ—Å', '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø–ª–∞–≤–ª–µ–Ω–∏—è',
                '–û–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è', '–¢–∏–ø –∫–æ–∂–∏', '–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å', '–°–æ—Å—Ç–∞–≤', '–°—Ç—Ä–∞–Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞',
                '–ë—Ä–µ–Ω–¥', '–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', '–ê—Ä—Ç–∏–∫—É–ª', '–ú–æ–¥–µ–ª—å', '–°–µ—Ä–∏—è', '–õ–∏–Ω–µ–π–∫–∞',
                '–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ü–∏—è', '–ê—Ä–æ–º–∞—Ç', '–¶–≤–µ—Ç', '–†–∞–∑–º–µ—Ä', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–£–ø–∞–∫–æ–≤–∫–∞',
                '–°—Ä–æ–∫ –≥–æ–¥–Ω–æ—Å—Ç–∏', '–£—Å–ª–æ–≤–∏—è —Ö—Ä–∞–Ω–µ–Ω–∏—è', '–°–ø–æ—Å–æ–± –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è', '–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è'
            ],
            'ua': [
                '–¢–∏–ø –∑–∞—Å–æ–±—É', '–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è', '–û–±\'—î–º', '–í–∞–≥–∞', '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø–ª–∞–≤–ª–µ–Ω–Ω—è',
                '–û–±–ª–∞—Å—Ç—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è', '–¢–∏–ø —à–∫—ñ—Ä–∏', '–°—É–º—ñ—Å–Ω—ñ—Å—Ç—å', '–°–∫–ª–∞–¥', '–ö—Ä–∞—ó–Ω–∞ –≤–∏—Ä–æ–±–Ω–∏—Ü—Ç–≤–∞',
                '–ë—Ä–µ–Ω–¥', '–í–∏—Ä–æ–±–Ω–∏–∫', '–ê—Ä—Ç–∏–∫—É–ª', '–ú–æ–¥–µ–ª—å', '–°–µ—Ä—ñ—è', '–õ—ñ–Ω—ñ–π–∫–∞',
                '–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ü—ñ—è', '–ê—Ä–æ–º–∞—Ç', '–ö–æ–ª—ñ—Ä', '–†–æ–∑–º—ñ—Ä', '–ö—ñ–ª—å–∫—ñ—Å—Ç—å', '–£–ø–∞–∫–æ–≤–∫–∞',
                '–¢–µ—Ä–º—ñ–Ω –ø—Ä–∏–¥–∞—Ç–Ω–æ—Å—Ç—ñ', '–£–º–æ–≤–∏ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è', '–°–ø–æ—Å—ñ–± –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è', '–Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è'
            ]
        }
        
        if locale not in whitelist:
            logger.error(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –ª–æ–∫–∞–ª—å –¥–ª—è LLM-–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏: {locale}")
            return {'success': False, 'reason': 'unsupported_locale', 'localized_specs': specs}
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º prompt –¥–ª—è LLM
        allowed_keys = ', '.join(whitelist[locale])
        specs_text = '\n'.join([f"{spec.get('name', '')}: {spec.get('value', '')}" for spec in specs])
        
        prompt = f"""–ü—Ä–∏–≤–µ–¥–∏ –¢–û–õ–¨–ö–û –∫–ª—é—á–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∫ –ª–æ–∫–∞–ª–∏ {locale} –ø–æ —ç—Ç–æ–º—É whitelist, –∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ —Ç—Ä–æ–≥–∞–π.

–î–û–ü–£–°–¢–ò–ú–´–ï –ö–õ–Æ–ß–ò: {allowed_keys}

–¢–ï–ö–£–©–ò–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò:
{specs_text}

–ü–†–ê–í–ò–õ–ê:
1. –ï—Å–ª–∏ –∫–ª—é—á –≤ whitelist - –æ—Å—Ç–∞–≤—å –∫–∞–∫ –µ—Å—Ç—å
2. –ï—Å–ª–∏ –∫–ª—é—á –ø–æ—Ö–æ–∂ –Ω–∞ –¥–æ–ø—É—Å—Ç–∏–º—ã–π - –∑–∞–º–µ–Ω–∏ –Ω–∞ –±–ª–∏–∂–∞–π—à–∏–π –∏–∑ whitelist
3. –ï—Å–ª–∏ –∫–ª—é—á –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç - –∏—Å–∫–ª—é—á–∏ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
4. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –º–∞—Å—Å–∏–≤ –æ–±—ä–µ–∫—Ç–æ–≤ {{"name": "–∫–ª—é—á", "value": "–∑–Ω–∞—á–µ–Ω–∏–µ"}}
5. –°–æ—Ö—Ä–∞–Ω–∏ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π

–û—Ç–≤–µ—Ç (—Ç–æ–ª—å–∫–æ JSON):"""

        try:
            # –í—ã–∑—ã–≤–∞–µ–º LLM —Å JSON-—Å—Ç—Ä–æ–≥–∏–º —Ä–µ–∂–∏–º–æ–º –∏ —Ä–µ—Ç—Ä–∞—è–º–∏
            from src.llm.content_generator import LLMContentGenerator
            llm_generator = LLMContentGenerator()
            
            # –î–æ–±–∞–≤–ª—è–µ–º —è–≤–Ω–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ JSON –≤ –ø—Ä–æ–º–ø—Ç
            json_prompt = prompt + "\n\n–í–ê–ñ–ù–û: –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –º–∞—Å—Å–∏–≤ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞!"
            
            # –ü–µ—Ä–≤–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å JSON-—Å—Ç—Ä–æ–≥–∏–º —Ä–µ–∂–∏–º–æ–º
            response = llm_generator.call_api_with_json_mode(json_prompt, max_tokens=1000, temperature=0.1)
            
            if not response:
                logger.error("‚ùå LLM –Ω–µ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç –¥–ª—è –ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–ª—é—á–µ–π")
                return {'success': False, 'reason': 'no_llm_response', 'localized_specs': specs}
            
            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
            import json
            try:
                localized_specs = json.loads(response)
                if not isinstance(localized_specs, list):
                    raise ValueError("–û—Ç–≤–µ—Ç –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –º–∞—Å—Å–∏–≤–æ–º")
            except (json.JSONDecodeError, ValueError) as e:
                logger.warning(f"‚ö†Ô∏è –ü–µ—Ä–≤–∞—è –ø–æ–ø—ã—Ç–∫–∞ LLM-–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                
                # –í—Ç–æ—Ä–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∏–º –ø—Ä–æ–º–ø—Ç–æ–º
                retry_prompt = f"""–ü–µ—Ä–µ–≤–µ–¥–∏ –∫–ª—é—á–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –Ω–∞ {locale} –∏ –≤–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –º–∞—Å—Å–∏–≤:

–î–û–ü–£–°–¢–ò–ú–´–ï –ö–õ–Æ–ß–ò: {', '.join(whitelist[locale])}

–•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò:
{specs_text}

–ü–†–ê–í–ò–õ–ê:
1. –ï—Å–ª–∏ –∫–ª—é—á –≤ whitelist - –æ—Å—Ç–∞–≤—å –∫–∞–∫ –µ—Å—Ç—å
2. –ï—Å–ª–∏ –∫–ª—é—á –ø–æ—Ö–æ–∂ - –∑–∞–º–µ–Ω–∏ –Ω–∞ –±–ª–∏–∂–∞–π—à–∏–π –∏–∑ whitelist  
3. –ï—Å–ª–∏ –∫–ª—é—á –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç - –∏—Å–∫–ª—é—á–∏
4. –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON –º–∞—Å—Å–∏–≤: [{{"name": "–∫–ª—é—á", "value": "–∑–Ω–∞—á–µ–Ω–∏–µ"}}]

JSON:"""
                
                retry_response = llm_generator.call_api_with_json_mode(retry_prompt, max_tokens=800, temperature=0.1)
                
                if not retry_response:
                    logger.error("‚ùå –†–µ—Ç—Ä–∞–π LLM-–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç")
                    return {'success': False, 'reason': 'llm_retry_no_response', 'localized_specs': specs}
                
                try:
                    localized_specs = json.loads(retry_response)
                    if not isinstance(localized_specs, list):
                        raise ValueError("–†–µ—Ç—Ä–∞–π –æ—Ç–≤–µ—Ç –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –º–∞—Å—Å–∏–≤–æ–º")
                    logger.info("‚úÖ –†–µ—Ç—Ä–∞–π LLM-–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ —É—Å–ø–µ—à–µ–Ω")
                except (json.JSONDecodeError, ValueError) as e:
                    logger.error(f"‚ùå –†–µ—Ç—Ä–∞–π LLM-–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ —É–¥–∞–ª—Å—è: {e}")
                    return {'success': False, 'reason': 'llm_invalid_json_retry_exhausted', 'localized_specs': specs}
            
            # –í–∞–ª–∏–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            valid_specs = []
            for spec in localized_specs:
                if isinstance(spec, dict) and 'name' in spec and 'value' in spec:
                    name = spec['name'].strip()
                    value = spec['value'].strip()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–ª—é—á –≤ whitelist
                    if name in whitelist[locale]:
                        valid_specs.append({'name': name, 'value': value})
                    else:
                        logger.warning(f"‚ö†Ô∏è LLM –≤–µ—Ä–Ω—É–ª –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –∫–ª—é—á: {name}")
            
            if len(valid_specs) < 3:
                logger.error(f"‚ùå –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –≤–∞–ª–∏–¥–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –ø–æ—Å–ª–µ LLM-–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏: {len(valid_specs)}")
                return {'success': False, 'reason': 'insufficient_specs', 'localized_specs': specs}
            
            logger.info(f"‚úÖ LLM-–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞: {len(valid_specs)} —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫")
            return {'success': True, 'localized_specs': valid_specs}
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM-–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            return {'success': False, 'reason': 'llm_error', 'localized_specs': specs}

    def deterministic_specs_normalize(self, specs: List[Dict[str, str]], locale: str) -> Dict[str, Any]:
        """
        –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è spec-–∫–ª—é—á–µ–π: –∑–∞–º–µ–Ω—è–µ—Ç UA-–ª–µ–π–±–ª—ã –Ω–∞ RU –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.
        """
        logger.info(f"üîß –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è spec-–∫–ª—é—á–µ–π –¥–ª—è {locale}")
        
        # –°–ª–æ–≤–∞—Ä—å –∑–∞–º–µ–Ω—ã UA->RU –∏ RU->UA
        normalization_map = {
            'ru': {
                '–¢–∏–ø –∑–∞—Å–æ–±—É': '–¢–∏–ø —Å—Ä–µ–¥—Å—Ç–≤–∞',
                '–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è': '–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ', 
                '–û–±\'—î–º': '–û–±—ä—ë–º',
                '–í–∞–≥–∞': '–í–µ—Å',
                '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø–ª–∞–≤–ª–µ–Ω–Ω—è': '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø–ª–∞–≤–ª–µ–Ω–∏—è',
                '–û–±–ª–∞—Å—Ç—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è': '–û–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è',
                '–¢–∏–ø —à–∫—ñ—Ä–∏': '–¢–∏–ø –∫–æ–∂–∏',
                '–°—É–º—ñ—Å–Ω—ñ—Å—Ç—å': '–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å',
                '–°–∫–ª–∞–¥': '–°–æ—Å—Ç–∞–≤',
                '–ö—Ä–∞—ó–Ω–∞ –≤–∏—Ä–æ–±–Ω–∏—Ü—Ç–≤–∞': '–°—Ç—Ä–∞–Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞',
                '–í–∏—Ä–æ–±–Ω–∏–∫': '–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å',
                '–°–µ—Ä—ñ—è': '–°–µ—Ä–∏—è',
                '–õ—ñ–Ω—ñ–π–∫–∞': '–õ–∏–Ω–µ–π–∫–∞',
                '–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ü—ñ—è': '–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ü–∏—è',
                '–ê—Ä–æ–º–∞—Ç': '–ê—Ä–æ–º–∞—Ç',
                '–ö–æ–ª—ñ—Ä': '–¶–≤–µ—Ç',
                '–†–æ–∑–º—ñ—Ä': '–†–∞–∑–º–µ—Ä',
                '–ö—ñ–ª—å–∫—ñ—Å—Ç—å': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ',
                '–£–ø–∞–∫–æ–≤–∫–∞': '–£–ø–∞–∫–æ–≤–∫–∞',
                '–¢–µ—Ä–º—ñ–Ω –ø—Ä–∏–¥–∞—Ç–Ω–æ—Å—Ç—ñ': '–°—Ä–æ–∫ –≥–æ–¥–Ω–æ—Å—Ç–∏',
                '–£–º–æ–≤–∏ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è': '–£—Å–ª–æ–≤–∏—è —Ö—Ä–∞–Ω–µ–Ω–∏—è',
                '–°–ø–æ—Å—ñ–± –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è': '–°–ø–æ—Å–æ–± –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è',
                '–Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è': '–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è'
            },
            'ua': {
                '–¢–∏–ø —Å—Ä–µ–¥—Å—Ç–≤–∞': '–¢–∏–ø –∑–∞—Å–æ–±—É',
                '–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ': '–ü—Ä–∏–∑–Ω–∞—á–µ–Ω–Ω—è',
                '–û–±—ä—ë–º': '–û–±\'—î–º', 
                '–í–µ—Å': '–í–∞–≥–∞',
                '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø–ª–∞–≤–ª–µ–Ω–∏—è': '–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø–ª–∞–≤–ª–µ–Ω–Ω—è',
                '–û–±–ª–∞—Å—Ç–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è': '–û–±–ª–∞—Å—Ç—ñ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è',
                '–¢–∏–ø –∫–æ–∂–∏': '–¢–∏–ø —à–∫—ñ—Ä–∏',
                '–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å': '–°—É–º—ñ—Å–Ω—ñ—Å—Ç—å',
                '–°–æ—Å—Ç–∞–≤': '–°–∫–ª–∞–¥',
                '–°—Ç—Ä–∞–Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞': '–ö—Ä–∞—ó–Ω–∞ –≤–∏—Ä–æ–±–Ω–∏—Ü—Ç–≤–∞',
                '–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å': '–í–∏—Ä–æ–±–Ω–∏–∫',
                '–°–µ—Ä–∏—è': '–°–µ—Ä—ñ—è',
                '–õ–∏–Ω–µ–π–∫–∞': '–õ—ñ–Ω—ñ–π–∫–∞',
                '–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ü–∏—è': '–ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ü—ñ—è',
                '–¶–≤–µ—Ç': '–ö–æ–ª—ñ—Ä',
                '–†–∞–∑–º–µ—Ä': '–†–æ–∑–º—ñ—Ä',
                '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ': '–ö—ñ–ª—å–∫—ñ—Å—Ç—å',
                '–°—Ä–æ–∫ –≥–æ–¥–Ω–æ—Å—Ç–∏': '–¢–µ—Ä–º—ñ–Ω –ø—Ä–∏–¥–∞—Ç–Ω–æ—Å—Ç—ñ',
                '–£—Å–ª–æ–≤–∏—è —Ö—Ä–∞–Ω–µ–Ω–∏—è': '–£–º–æ–≤–∏ –∑–±–µ—Ä—ñ–≥–∞–Ω–Ω—è',
                '–°–ø–æ—Å–æ–± –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è': '–°–ø–æ—Å—ñ–± –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è',
                '–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è': '–Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è'
            }
        }
        
        if locale not in normalization_map:
            logger.error(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –ª–æ–∫–∞–ª—å –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {locale}")
            return {'success': False, 'reason': 'unsupported_locale', 'normalized_specs': specs}
        
        normalized_specs = []
        fixed_count = 0
        dropped_count = 0
        
        for spec in specs:
            name = spec.get('name', '')
            value = spec.get('value', '')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–ª—é—á
            normalized_name = normalization_map[locale].get(name, name)
            
            if normalized_name != name:
                # –ó–∞–º–µ–Ω—è–µ–º –∫–ª—é—á
                normalized_specs.append({'name': normalized_name, 'value': value})
                fixed_count += 1
                logger.info(f"üîß –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω –∫–ª—é—á: '{name}' -> '{normalized_name}'")
            else:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã–º –∫–ª—é—á–æ–º
                if self._is_conflict_key(name, locale):
                    dropped_count += 1
                    logger.info(f"üîß –£–¥–∞–ª–µ–Ω –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã–π –∫–ª—é—á: '{name}'")
                else:
                    # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    normalized_specs.append(spec)
        
        if fixed_count > 0 or dropped_count > 0:
            logger.info(f"‚úÖ –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–æ {fixed_count}, —É–¥–∞–ª–µ–Ω–æ {dropped_count} –∫–ª—é—á–µ–π")
        
        return {
            'success': True,
            'normalized_specs': normalized_specs,
            'fixed_count': fixed_count,
            'dropped_count': dropped_count
        }
    
    def _is_conflict_key(self, name: str, locale: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–ª—é—á –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã–º –¥–ª—è –¥–∞–Ω–Ω–æ–π –ª–æ–∫–∞–ª–∏"""
        conflict_patterns = {
            'ru': ['–≥–∞—Ä—è—á–∏–π', '–æ–±–ª–∏—á—á—è', '–æ–±–ª–∞—Å—Ç—å –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø–ª–∞–≤–ª–µ–Ω–Ω—è', 
                   '–≤–∞–≥–∞', '–æ–±\'—î–º', '–º–∞—Ç–µ—Ä—ñ–∞–ª', '–∫–æ–ª—ñ—Ä', '—Å–µ—Ä—ñ—è', '–∑–æ–Ω–∏', '—Ç–∏–ø —à–∫—ñ—Ä–∏'],
            'ua': ['–≥–æ—Ä—è—á–∏–π', '–ª–∏—Ü–µ', '–æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø–ª–∞–≤–ª–µ–Ω–∏—è',
                   '–≤–µ—Å', '–æ–±—ä–µ–º', '–º–∞—Ç–µ—Ä–∏–∞–ª', '—Ü–≤–µ—Ç', '—Å–µ—Ä–∏—è', '–∑–æ–Ω—ã', '—Ç–∏–ø –∫–æ–∂–∏']
        }
        
        if locale not in conflict_patterns:
            return False
        
        name_lower = name.lower()
        for pattern in conflict_patterns[locale]:
            if pattern in name_lower:
                return True
        
        return False

    def deterministic_specs_drop(self, specs: List[Dict[str, str]], locale: str) -> List[Dict[str, str]]:
        """
        –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π fallback: —É–¥–∞–ª—è–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã–µ –∫–ª—é—á–∏ –∏–∑ specs.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ LLM-–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –∏–ª–∏ –Ω–µ —É–¥–∞–ª–∞—Å—å.
        """
        logger.info(f"üîß –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥—Ä–æ–ø –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã—Ö –∫–ª—é—á–µ–π –¥–ª—è {locale}")
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã—Ö –∫–ª—é—á–µ–π
        conflict_patterns = {
            'ru': ['–∫–ª–∞—Å', '–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è', '—Ç–∏–ø', '–≤–∏–¥'],  # UA-–ª–µ–∫—Å–µ–º—ã –≤ RU
            'ua': ['–∫–ª–∞—Å—Å', '–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è', '—Ç–∏–ø', '–≤–∏–¥']   # RU-–ª–µ–∫—Å–µ–º—ã –≤ UA
        }
        
        if locale not in conflict_patterns:
            return specs
        
        filtered_specs = []
        dropped_count = 0
        
        for spec in specs:
            name = spec.get('name', '').lower()
            should_drop = False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern in conflict_patterns[locale]:
                if pattern in name:
                    should_drop = True
                    break
            
            if should_drop:
                dropped_count += 1
                logger.info(f"üîß –£–¥–∞–ª–µ–Ω –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã–π –∫–ª—é—á: {spec.get('name', '')}")
            else:
                filtered_specs.append(spec)
        
        if dropped_count > 0:
            logger.info(f"‚úÖ –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥—Ä–æ–ø: —É–¥–∞–ª–µ–Ω–æ {dropped_count} –∫–æ–Ω—Ñ–ª–∏–∫—Ç–Ω—ã—Ö –∫–ª—é—á–µ–π")
        
        return filtered_specs

    def ensure_min_sentences(self, description: str, locale: str, target: int = 5) -> Dict[str, Any]:
        """
        –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ –æ–ø–∏—Å–∞–Ω–∏–∏.
        –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –µ—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç.
        """
        logger.info(f"üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –¥–ª—è {locale} (—Ü–µ–ª—å: {target})")
        
        current_sentences = self._count_sentences(description)
        if current_sentences >= target:
            logger.info(f"‚úÖ –£–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {current_sentences}")
            return {
                'success': True,
                'reason': 'already_sufficient',
                'fixed_description': description,
                'sentences_added': 0
            }
        
        sentences_needed = target - current_sentences
        selected_sentences = self._select_sentences(locale, sentences_needed)
        
        if not selected_sentences:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–±—Ä–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è {locale}")
            return {
                'success': False,
                'reason': 'no_sentences_available',
                'fixed_description': description
            }
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–∫—Å
        fixed_description = self._apply_fix(description, selected_sentences)
        new_sentences = self._count_sentences(fixed_description)
        
        logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {len(selected_sentences)} (–±—ã–ª–æ: {current_sentences}, —Å—Ç–∞–ª–æ: {new_sentences})")
        
        return {
            'success': True,
            'reason': 'sentences_added',
            'fixed_description': fixed_description,
            'sentences_added': len(selected_sentences),
            'original_sentences': current_sentences,
            'new_sentences': new_sentences
        }

    def ensure_min_chars(self, description: str, locale: str, target: int = 450) -> Dict[str, Any]:
        """
        –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –æ–ø–∏—Å–∞–Ω–∏—è –≤ —Å–∏–º–≤–æ–ª–∞—Ö.
        –î–æ–±–∞–≤–ª—è–µ—Ç –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –µ—Å–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç.
        """
        logger.info(f"üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã –¥–ª—è {locale} (—Ü–µ–ª—å: {target} —Å–∏–º–≤–æ–ª–æ–≤)")
        
        current_length = len(description.strip())
        if current_length >= target:
            logger.info(f"‚úÖ –£–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤: {current_length}")
            return {
                'success': True,
                'reason': 'already_sufficient',
                'fixed_description': description,
                'chars_added': 0
            }
        
        # –í—ã–±–∏—Ä–∞–µ–º –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        available_sentences = self.safe_sentences.get(locale, [])
        selected_sentences = []
        current_chars = current_length
        
        for sentence in available_sentences:
            if current_chars >= target:
                break
            selected_sentences.append(sentence)
            current_chars += len(sentence) + 2  # +2 –¥–ª—è —Ç–æ—á–∫–∏ –∏ –ø—Ä–æ–±–µ–ª–∞
        
        if not selected_sentences:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–±—Ä–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è {locale}")
            return {
                'success': False,
                'reason': 'no_sentences_available',
                'fixed_description': description
            }
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–∫—Å
        fixed_description = self._apply_fix(description, selected_sentences)
        new_length = len(fixed_description.strip())
        chars_added = new_length - current_length
        
        logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤: {chars_added} (–±—ã–ª–æ: {current_length}, —Å—Ç–∞–ª–æ: {new_length})")
        
        return {
            'success': True,
            'reason': 'chars_added',
            'fixed_description': fixed_description,
            'chars_added': chars_added,
            'original_length': current_length,
            'new_length': new_length
        }

    def normalize_title(self, llm_content: Dict[str, Any], locale: str, product_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ - —Å–æ–∑–¥–∞–µ—Ç –∏–∑ facts –µ—Å–ª–∏ –ø—É—Å—Ç/–∫–æ—Ä–æ—Ç–∫–∏–π.
        """
        logger.info(f"üîß –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–ª—è {locale}")
        
        current_title = llm_content.get('title', '').strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É –∑–∞–≥–æ–ª–æ–≤–∫–∞
        if len(current_title) >= 10:
            logger.info(f"‚úÖ –ó–∞–≥–æ–ª–æ–≤–æ–∫ —É–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –¥–ª–∏–Ω—ã: {len(current_title)} —Å–∏–º–≤–æ–ª–æ–≤")
            return llm_content
        
        logger.warning(f"‚ö†Ô∏è –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π: {len(current_title)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ facts
        new_title = self._create_title_from_facts(locale, product_data, llm_content)
        
        if new_title and len(new_title) >= 10:
            logger.info(f"üîß –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: {new_title}")
            llm_content['title'] = new_title
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–ª–∞–≥ –≤ issues
            if 'issues' not in llm_content:
                llm_content['issues'] = []
            llm_content['issues'].append('repair: title_sanitized_safe_constructor')
        else:
            logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≤–∞–ª–∏–¥–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è {locale}")
        
        return llm_content

    def _create_title_from_facts(self, locale: str, product_data: Dict[str, Any], llm_content: Dict[str, Any]) -> str:
        """
        –°–æ–∑–¥–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö facts (—Ñ–æ—Ä–º–∞ + –±—Ä–µ–Ω–¥ + —Å–µ—Ä–∏—è/–∞—Ä–æ–º–∞—Ç + –æ–±—ä—ë–º).
        """
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ product_data
        url = product_data.get('url', '')
        specs = llm_content.get('specs', [])
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç–æ–≤–∞—Ä–∞
        product_type = self._extract_product_type(url, locale)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –±—Ä–µ–Ω–¥, –∞—Ä–æ–º–∞—Ç, –æ–±—ä–µ–º –∏–∑ specs
        brand = self._extract_from_specs(specs, ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å'])
        aroma = self._extract_from_specs(specs, ['–∞—Ä–æ–º–∞—Ç', 'aroma', '–∑–∞–ø–∞—Ö', '–∑–∞–ø–∞—Ö'])
        volume = self._extract_from_specs(specs, ['–æ–±—ä–µ–º', '–æ–±\'—î–º', 'volume', '–º–ª', 'ml'])
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title_parts = []
        
        if product_type:
            title_parts.append(product_type)
        
        if brand:
            title_parts.append(brand)
        
        if aroma:
            title_parts.append(aroma)
        
        if volume:
            title_parts.append(volume)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞—Å—Ç–∏
        if title_parts:
            if locale == 'ru':
                return ', '.join(title_parts)
            else:
                return ', '.join(title_parts)
        
        # Fallback - –∏—Å–ø–æ–ª—å–∑—É–µ–º URL
        return self._extract_title_from_url(url, locale)

    def _extract_product_type(self, url: str, locale: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–∏–ø —Ç–æ–≤–∞—Ä–∞ –∏–∑ URL"""
        url_lower = url.lower()
        
        if any(word in url_lower for word in ['visk', 'wax']):
            return '–í—ñ—Å–∫' if locale == 'ua' else '–í–æ—Å–∫'
        elif any(word in url_lower for word in ['losion', 'lotion']):
            return '–õ–æ—Å—å–π–æ–Ω' if locale == 'ua' else '–õ–æ—Å—å–æ–Ω'
        elif any(word in url_lower for word in ['ochysnyk', 'ochistitel', 'cleaner']):
            return '–û—á–∏—Å–Ω–∏–∫' if locale == 'ua' else '–û—á–∏—Å—Ç–∏—Ç–µ–ª—å'
        elif any(word in url_lower for word in ['gel', '–≥–µ–ª—å']):
            return '–ì–µ–ª—å' if locale == 'ua' else '–ì–µ–ª—å'
        elif any(word in url_lower for word in ['foam', '–ø—ñ–Ω–∞', '–ø–µ–Ω–∞']):
            return '–ü—ñ–Ω–∞' if locale == 'ua' else '–ü–µ–Ω–∞'
        
        return ''

    def _extract_from_specs(self, specs: List[Dict[str, str]], keywords: List[str]) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ specs –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        if not specs:
            return ''
        
        for spec in specs:
            if isinstance(spec, dict):
                name = spec.get('name', '').lower()
                value = spec.get('value', '').strip()
                
                if value and any(keyword in name for keyword in keywords):
                    return value
        
        return ''

    def _extract_title_from_url(self, url: str, locale: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ URL –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π fallback"""
        # –£–±–∏—Ä–∞–µ–º –¥–æ–º–µ–Ω –∏ –ø—É—Ç—å
        url_parts = url.split('/')
        if url_parts:
            last_part = url_parts[-1].replace('-', ' ').replace('_', ' ')
            # –ö–∞–ø–∏—Ç–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ
            words = last_part.split()
            if words:
                words[0] = words[0].capitalize()
                return ' '.join(words)
        
        return f'–¢–æ–≤–∞—Ä ({locale})'

    def generate_strict_description_with_llm(self, product_data: Dict[str, Any], locale: str, 
                                           previous_attempts: List[str] = None) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ —á–µ—Ä–µ–∑ LLM —Å –∂–µ—Å—Ç–∫–∏–º –ø—Ä–æ–º–ø—Ç–æ–º –Ω–∞ ‚â•5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏ ‚â•450 —Å–∏–º–≤–æ–ª–æ–≤.
        """
        logger.info(f"üîß –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä–æ–≥–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è —á–µ—Ä–µ–∑ LLM –¥–ª—è {locale}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Ç–æ–≤–∞—Ä–µ
        url = product_data.get('url', '')
        title = product_data.get('title', '')
        specs = product_data.get('specs', [])
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç–æ–≤–∞—Ä–∞
        product_type = self._extract_product_type(url, locale)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
        brand = self._extract_from_specs(specs, ['–±—Ä–µ–Ω–¥', 'brand', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å'])
        volume = self._extract_from_specs(specs, ['–æ–±—ä–µ–º', '–æ–±\'—î–º', 'volume', '–º–ª', 'ml'])
        aroma = self._extract_from_specs(specs, ['–∞—Ä–æ–º–∞—Ç', 'aroma', '–∑–∞–ø–∞—Ö'])
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä–æ–≥–∏–π –ø—Ä–æ–º–ø—Ç
        prompt_parts = []
        
        if locale == 'ru':
            prompt_parts.append(f"–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –æ–ø–∏—Å–∞–Ω–∏—è –¥–ª—è {product_type or '—Ç–æ–≤–∞—Ä–∞'}")
            if brand:
                prompt_parts.append(f"–±—Ä–µ–Ω–¥–∞ {brand}")
            if volume:
                prompt_parts.append(f"–æ–±—ä—ë–º–æ–º {volume}")
            if aroma:
                prompt_parts.append(f"—Å –∞—Ä–æ–º–∞—Ç–æ–º {aroma}")
            
            prompt_parts.append("–¢–†–ï–ë–û–í–ê–ù–ò–Ø:")
            prompt_parts.append("- —Ä–æ–≤–Ω–æ 5-6 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π")
            prompt_parts.append("- –º–∏–Ω–∏–º—É–º 450 —Å–∏–º–≤–æ–ª–æ–≤")
            prompt_parts.append("- –∑–∞–ø—Ä–µ—Ç—ã: —Ü–µ–Ω—ã/–∞–∫—Ü–∏–∏/–¥–æ—Å—Ç–∞–≤–∫–∞")
            prompt_parts.append("- —Å—Ç–∏–ª—å: –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π")
            prompt_parts.append("- –≤–∫–ª—é—á–∏: –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ü–∏—é, –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∑–æ–Ω—ã, —Ç–∏–ø—ã –∫–æ–∂–∏, —Å–ø–æ—Å–æ–± –Ω–∞–Ω–µ—Å–µ–Ω–∏—è")
            prompt_parts.append("- –ª–æ–∫–∞–ª—å: —Ä—É—Å—Å–∫–∏–π")
        else:
            prompt_parts.append(f"–°–≥–µ–Ω–µ—Ä—É–π –æ–ø–∏—Å –¥–ª—è {product_type or '—Ç–æ–≤–∞—Ä—É'}")
            if brand:
                prompt_parts.append(f"–±—Ä–µ–Ω–¥—É {brand}")
            if volume:
                prompt_parts.append(f"–æ–±'—î–º–æ–º {volume}")
            if aroma:
                prompt_parts.append(f"–∑ –∞—Ä–æ–º–∞—Ç–æ–º {aroma}")
            
            prompt_parts.append("–í–ò–ú–û–ì–ò:")
            prompt_parts.append("- —Ä—ñ–≤–Ω–æ 5-6 —Ä–µ—á–µ–Ω—å")
            prompt_parts.append("- –º—ñ–Ω—ñ–º—É–º 450 —Å–∏–º–≤–æ–ª—ñ–≤")
            prompt_parts.append("- –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–æ: —Ü—ñ–Ω–∏/–∞–∫—Ü—ñ—ó/–¥–æ—Å—Ç–∞–≤–∫–∞")
            prompt_parts.append("- —Å—Ç–∏–ª—å: —ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–∏–π")
            prompt_parts.append("- –≤–∫–ª—é—á–∏: –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ü—ñ—é, –ø—ñ–¥—Ö–æ–¥—è—â—ñ –∑–æ–Ω–∏, —Ç–∏–ø–∏ —à–∫—ñ—Ä–∏, —Å–ø–æ—Å—ñ–± –Ω–∞–Ω–µ—Å–µ–Ω–Ω—è")
            prompt_parts.append("- –ª–æ–∫–∞–ª—å: —É–∫—Ä–∞—ó–Ω—Å—å–∫–∞")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–ø—ã—Ç–∫–∞—Ö
        if previous_attempts:
            if locale == 'ru':
                prompt_parts.append("–ü–†–ï–î–´–î–£–©–ò–ï –ü–û–ü–´–¢–ö–ò (–Ω–µ –ø–æ–≤—Ç–æ—Ä—è–π):")
            else:
                prompt_parts.append("–ü–û–ü–ï–†–ï–î–ù–Ü –°–ü–†–û–ë–ò (–Ω–µ –ø–æ–≤—Ç–æ—Ä—é–π):")
            
            for i, attempt in enumerate(previous_attempts[-2:], 1):  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 2 –ø–æ–ø—ã—Ç–∫–∏
                sentences = self._count_sentences(attempt)
                chars = len(attempt.strip())
                prompt_parts.append(f"–ü–æ–ø—ã—Ç–∫–∞ {i}: {sentences} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, {chars} —Å–∏–º–≤–æ–ª–æ–≤")
        
        prompt = '\n'.join(prompt_parts)
        
        try:
            # –í—ã–∑—ã–≤–∞–µ–º LLM
            from src.llm.content_generator import LLMContentGenerator
            llm_generator = LLMContentGenerator()
            
            response = llm_generator.call_api(prompt, max_tokens=800, temperature=0.3)
            
            if not response:
                logger.error("‚ùå LLM –Ω–µ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç –¥–ª—è —Å—Ç—Ä–æ–≥–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è")
                return {
                    'success': False,
                    'reason': 'no_llm_response',
                    'description': ''
                }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            sentences = self._count_sentences(response)
            chars = len(response.strip())
            
            logger.info(f"üîç LLM —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {sentences} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, {chars} —Å–∏–º–≤–æ–ª–æ–≤")
            
            if sentences >= 5 and chars >= 450:
                logger.info(f"‚úÖ –°—Ç—Ä–æ–≥–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º")
                return {
                    'success': True,
                    'description': response.strip(),
                    'sentences': sentences,
                    'chars': chars
                }
            else:
                logger.warning(f"‚ö†Ô∏è –°—Ç—Ä–æ–≥–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º: {sentences} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, {chars} —Å–∏–º–≤–æ–ª–æ–≤")
                return {
                    'success': False,
                    'reason': 'insufficient_quality',
                    'description': response.strip(),
                    'sentences': sentences,
                    'chars': chars
                }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å—Ç—Ä–æ–≥–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è: {e}")
            return {
                'success': False,
                'reason': 'llm_error',
                'description': ''
            }
