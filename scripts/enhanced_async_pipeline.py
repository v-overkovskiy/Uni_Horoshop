"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ FAQ –∏ JSON-LD
"""
import asyncio
import httpx
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import sys
import os
from dotenv import load_dotenv

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.core.async_product_processor import AsyncProductProcessor
from src.export.async_exporter import AsyncExporter
from src.monitoring.progress_monitor import ProgressMonitor
from src.processing.json_ld_generator import JsonLdGenerator

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê–õ–¨–ù–ê–Ø –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
CONCURRENT_PRODUCTS = 8   # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º
LLM_CONCURRENCY = 16      # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ LLM –∑–∞–ø—Ä–æ—Å–æ–≤
TIMEOUT = 45             # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
MAX_RETRIES = 2          # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö

class EnhancedAsyncPipeline:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ FAQ"""
    
    def __init__(self):
        self.processor = AsyncProductProcessor()
        self.exporter = AsyncExporter()
        self.monitor = None  # –ë—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ–∑–∂–µ
        self.json_ld_gen = JsonLdGenerator()
        
        # –°–µ–º–∞—Ñ–æ—Ä—ã –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–≥—Ä—É–∑–∫–∏
        self.llm_semaphore = asyncio.Semaphore(LLM_CONCURRENCY)
        self.product_semaphore = asyncio.Semaphore(CONCURRENT_PRODUCTS)
        
        self.results = []
        self.errors = []
        
    async def load_urls_from_file(self, filename: str = "urls.txt") -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ URL –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                urls = [line.strip() for line in f if line.strip()]
            logger.info(f"üìã –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(urls)} URL –∏–∑ —Ñ–∞–π–ª–∞ {filename}")
            return urls
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ URL: {e}")
            return []
    
    async def process_product_worker(
        self,
        product_url: str,
        input_index: int,
        client: httpx.AsyncClient, 
        llm_semaphore: asyncio.Semaphore, 
        exporter: AsyncExporter,
        monitor: ProgressMonitor = None
    ) -> Optional[Dict[str, Any]]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞ —Å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ FAQ"""
        async with self.product_semaphore:
            try:
                logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É: {product_url}")
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–≤–∞—Ä —á–µ—Ä–µ–∑ AsyncProductProcessor —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
                write_lock = asyncio.Lock()
                result = await self.processor.process_product_with_validation(
                    product_url, 
                    client, 
                    llm_semaphore, 
                    write_lock
                )
                
                if result and result.get('status') == 'success':
                    # –î–æ–±–∞–≤–ª—è–µ–º input_index –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
                    result['input_index'] = input_index
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º JSON-LD —Ä–∞–∑–º–µ—Ç–∫—É –¥–ª—è FAQ
                    await self._add_json_ld_to_result(result)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    await exporter.add_result(result)
                    
                    if monitor:
                        monitor.update_progress(1)
                    
                    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {product_url} (index {input_index})")
                    return result
                else:
                    error_msg = result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞') if result else '–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞'
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {product_url}: {error_msg}")
                    self.errors.append({
                        'url': product_url,
                        'input_index': input_index,
                        'error': error_msg,
                        'timestamp': datetime.now().isoformat()
                    })
                    return None
                    
            except Exception as e:
                logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {product_url} (index {input_index}): {e}")
                self.errors.append({
                    'url': product_url,
                    'input_index': input_index,
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                })
                return None
    
    async def _add_json_ld_to_result(self, result: Dict[str, Any]) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç JSON-LD —Ä–∞–∑–º–µ—Ç–∫—É –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º FAQ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            ru_faq = result.get('ru_content', {}).get('faq', [])
            ua_faq = result.get('ua_content', {}).get('faq', [])
            
            ru_title = result.get('ru_content', {}).get('title', '')
            ua_title = result.get('ua_content', {}).get('title', '')
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º JSON-LD –¥–ª—è RU
            if ru_faq and ru_title:
                ru_json_ld = self.json_ld_gen.generate_faq_schema(
                    faq_list=ru_faq,
                    product_name=ru_title,
                    locale='ru'
                )
                if ru_json_ld:
                    result['ru_json_ld'] = ru_json_ld
                    logger.info(f"‚úÖ JSON-LD –¥–æ–±–∞–≤–ª–µ–Ω –¥–ª—è RU: {len(ru_json_ld)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º JSON-LD –¥–ª—è UA
            if ua_faq and ua_title:
                ua_json_ld = self.json_ld_gen.generate_faq_schema(
                    faq_list=ua_faq,
                    product_name=ua_title,
                    locale='ua'
                )
                if ua_json_ld:
                    result['ua_json_ld'] = ua_json_ld
                    logger.info(f"‚úÖ JSON-LD –¥–æ–±–∞–≤–ª–µ–Ω –¥–ª—è UA: {len(ua_json_ld)} —Å–∏–º–≤–æ–ª–æ–≤")
                    
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è JSON-LD: {e}")
    
    async def process_urls(self, urls: List[str]) -> List[Dict[str, Any]]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ URL —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º"""
        logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {len(urls)} —Ç–æ–≤–∞—Ä–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (input_index, url) –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞
        indexed_urls = [(i + 1, url) for i, url in enumerate(urls)]
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–Ω–∏—Ç–æ—Ä —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–æ–≤–∞—Ä–æ–≤
        self.monitor = ProgressMonitor(total_products=len(indexed_urls))
        
        # –°–æ–∑–¥–∞–µ–º HTTP –∫–ª–∏–µ–Ω—Ç —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        timeout = httpx.Timeout(TIMEOUT, connect=10.0)
        limits = httpx.Limits(max_keepalive_connections=20, max_connections=100)
        
        async with httpx.AsyncClient(timeout=timeout, limits=limits) as client:
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏
            tasks = []
            for input_index, url in indexed_urls:
                task = asyncio.create_task(
                    self.process_product_worker(
                        product_url=url,
                        input_index=input_index,
                        client=client,
                        llm_semaphore=self.llm_semaphore,
                        exporter=self.exporter,
                        monitor=self.monitor
                    )
                )
                tasks.append(task)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            logger.info(f"‚ö° –ó–∞–ø—É—Å–∫–∞–µ–º {len(tasks)} –∑–∞–¥–∞—á –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏")
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—É—Å–ø–µ—à–Ω—ã–µ + –æ—à–∏–±–æ—á–Ω—ã–µ) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º input_index
            all_results = []
            successful_results = []
            
            for i, result in enumerate(results):
                input_index = indexed_urls[i][0]
                url = indexed_urls[i][1]
                
                if isinstance(result, Exception):
                    logger.error(f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –≤ –∑–∞–¥–∞—á–µ {i} (index {input_index}): {result}")
                    error_data = {
                        'url': url,
                        'input_index': input_index,
                        'error': str(result),
                        'timestamp': datetime.now().isoformat(),
                        'status': 'error',
                        'ru_html': '',
                        'ua_html': '',
                        'ru_title': '',
                        'ua_title': '',
                        'ru_hero_image': '',
                        'ua_hero_image': '',
                        'processing_time': 0.0,
                        'errors': str(result),
                        'budget_stats': '',
                        'adapter_version': '2.0',
                        'hero_quality': 0.0,
                        'calls_per_locale': 0,
                        'canonical_slug': '',
                        'ru_valid': False,
                        'ua_valid': False
                    }
                    all_results.append(error_data)
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–≤–∞—Ä —Å –æ—à–∏–±–∫–æ–π –≤ —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä
                    await self.exporter.add_result(error_data)
                    self.errors.append({
                        'url': url,
                        'input_index': input_index,
                        'error': str(result),
                        'timestamp': datetime.now().isoformat()
                    })
                elif result is not None:
                    # –î–æ–±–∞–≤–ª—è–µ–º input_index –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –∏ –æ—Ç–º–µ—á–∞–µ–º –∫–∞–∫ —É—Å–ø–µ—à–Ω—ã–π
                    result['input_index'] = input_index
                    result['status'] = 'success'
                    all_results.append(result)
                    successful_results.append(result)
                else:
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ª—É—á–∞–π, –∫–æ–≥–¥–∞ result is None
                    logger.error(f"‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç None –¥–ª—è –∑–∞–¥–∞—á–∏ {i} (index {input_index}): {url}")
                    error_data = {
                        'url': url,
                        'input_index': input_index,
                        'error': '–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–≤–µ–Ω None',
                        'timestamp': datetime.now().isoformat(),
                        'status': 'error',
                        'ru_html': '',
                        'ua_html': '',
                        'ru_title': '',
                        'ua_title': '',
                        'ru_hero_image': '',
                        'ua_hero_image': '',
                        'processing_time': 0.0,
                        'errors': '–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–≤–µ–Ω None',
                        'budget_stats': '',
                        'adapter_version': '2.0',
                        'hero_quality': 0.0,
                        'calls_per_locale': 0,
                        'canonical_slug': '',
                        'ru_valid': False,
                        'ua_valid': False
                    }
                    all_results.append(error_data)
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–≤–∞—Ä —Å –æ—à–∏–±–∫–æ–π –≤ —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä
                    await self.exporter.add_result(error_data)
                    self.errors.append({
                        'url': url,
                        'input_index': input_index,
                        'error': '–†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–∞–≤–µ–Ω None',
                        'timestamp': datetime.now().isoformat()
                    })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –í–°–ï —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ input_index –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞
            all_results.sort(key=lambda x: x.get('input_index', 0))
            successful_results.sort(key=lambda x: x.get('input_index', 0))
            
            # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –û–ë–ù–û–í–õ–Ø–ï–ú —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–º–µ—Å—Ç–æ –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏
            # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–∞—É–Ω–¥–æ–≤
            for new_result in all_results:
                url = new_result.get('url')
                # –ò—â–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —ç—Ç–æ–≥–æ URL
                existing_index = next((i for i, r in enumerate(self.results) if r.get('url') == url), None)
                
                if existing_index is not None:
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    self.results[existing_index] = new_result
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    self.results.append(new_result)
            
            logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(successful_results)} —É—Å–ø–µ—à–Ω—ã—Ö –∏–∑ {len(all_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ input_index")
            logger.info(f"üìä –í—Å–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –±–∞–∑–µ: {len(self.results)}")
            return all_results  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞—É–Ω–¥–∞
    
    def get_failed_urls(self) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç URL —Ç–æ–≤–∞—Ä–æ–≤ —Å–æ —Å—Ç–∞—Ç—É—Å–æ–º 'error' –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        failed_urls = []
        for result in self.results:
            if result.get('status') == 'error':
                url = result.get('url', '')
                if url and url not in failed_urls:
                    failed_urls.append(url)
        
        logger.info(f"üîÑ –ù–∞–π–¥–µ–Ω–æ {len(failed_urls)} —Ç–æ–≤–∞—Ä–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏ –¥–ª—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return failed_urls
    
    def clear_errors_for_urls(self, urls: List[str]) -> None:
        """–û—á–∏—â–∞–µ—Ç –¢–û–õ–¨–ö–û –æ—à–∏–±–∫–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö URL –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        
        –í–ê–ñ–ù–û: –ù–ï —É–¥–∞–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ self.results, —Ç–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏!
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –∑–∞–º–µ–Ω–µ–Ω—ã –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–π –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–µ —á–µ—Ä–µ–∑ add_result –≤ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ
        """
        # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ –æ–± –æ—à–∏–±–∫–∞—Ö –¥–ª—è —ç—Ç–∏—Ö URL
        self.errors = [e for e in self.errors if e.get('url') not in urls]
        
        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–û: –ù–ï —É–¥–∞–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã! –¢–æ–ª—å–∫–æ –ø–æ–º–µ—á–∞–µ–º –¥–ª—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Å—Ç–∞–Ω—É—Ç—Å—è –≤ self.results –∏ –±—É–¥—É—Ç –æ–±–Ω–æ–≤–ª–µ–Ω—ã –µ—Å–ª–∏ —Ç–æ–≤–∞—Ä —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç—Å—è
        
        logger.info(f"üßπ –û—á–∏—â–µ–Ω—ã –æ—à–∏–±–∫–∏ –¥–ª—è {len(urls)} —Ç–æ–≤–∞—Ä–æ–≤ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è)")
    
    def print_statistics(self) -> None:
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        total_processed = len(self.results)
        total_errors = len(self.errors)
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ª–æ–≥–∏–∫—É –ø–æ–¥—Å—á–µ—Ç–∞ - –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL —ç—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        total_urls = total_processed
        
        logger.info("=" * 60)
        logger.info("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò")
        logger.info("=" * 60)
        # –°—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (—Å —Å—Ç–∞—Ç—É—Å–æ–º 'success')
        successful_count = sum(1 for r in self.results if r.get('status') == 'success')
        error_count = sum(1 for r in self.results if r.get('status') == 'error')
        
        logger.info(f"–í—Å–µ–≥–æ URL: {total_urls}")
        logger.info(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful_count}")
        logger.info(f"–û—à–∏–±–æ–∫: {error_count}")
        logger.info(f"–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {(successful_count/total_urls*100):.1f}%" if total_urls > 0 else "0%")
        
        # ‚úÖ –ù–û–í–û–ï: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–æ–¥–µ–ª—è–º
        model_stats = {}
        for r in self.results:
            model = r.get('processed_by_model', 'unknown')
            model_stats[model] = model_stats.get(model, 0) + 1
        
        if model_stats:
            logger.info(f"\nü§ñ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ú–û–î–ï–õ–Ø–ú:")
            for model, count in sorted(model_stats.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / successful_count * 100) if successful_count > 0 else 0
                # –í—ã–¥–µ–ª—è–µ–º –¥–æ—Ä–æ–≥—É—é –º–æ–¥–µ–ª—å GPT-4o
                if 'gpt-4o' in model.lower() and 'mini' not in model.lower():
                    logger.info(f"  üí∞ {model}: {count} —Ç–æ–≤–∞—Ä–æ–≤ ({percentage:.1f}%) ‚ö†Ô∏è –î–û–†–û–ì–ê–Ø –ú–û–î–ï–õ–¨")
                else:
                    logger.info(f"  ü§ñ {model}: {count} —Ç–æ–≤–∞—Ä–æ–≤ ({percentage:.1f}%)")
        
        if self.errors:
            logger.info("\n‚ùå –û–®–ò–ë–ö–ò:")
            for error in self.errors:
                logger.info(f"  - {error['url']}: {error['error']}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ FAQ
        total_faq_ru = sum(len(r.get('ru_content', {}).get('faq', [])) for r in self.results)
        total_faq_ua = sum(len(r.get('ua_content', {}).get('faq', [])) for r in self.results)
        logger.info(f"\nüìù FAQ –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        logger.info(f"  RU FAQ: {total_faq_ru} –≤–æ–ø—Ä–æ—Å–æ–≤")
        logger.info(f"  UA FAQ: {total_faq_ua} –≤–æ–ø—Ä–æ—Å–æ–≤")
        logger.info(f"  –í—Å–µ–≥–æ FAQ: {total_faq_ru + total_faq_ua} –≤–æ–ø—Ä–æ—Å–æ–≤")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ JSON-LD
        json_ld_count = sum(1 for r in self.results if r.get('ru_json_ld') or r.get('ua_json_ld'))
        logger.info(f"\nüè∑Ô∏è JSON-LD –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        logger.info(f"  –¢–æ–≤–∞—Ä–æ–≤ —Å JSON-LD: {json_ld_count}")
        logger.info(f"  –ü–æ–∫—Ä—ã—Ç–∏–µ JSON-LD: {(json_ld_count/total_processed*100):.1f}%" if total_processed > 0 else "0%")

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    start_time = datetime.now()
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
    pipeline = EnhancedAsyncPipeline()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º URL
    urls = await pipeline.load_urls_from_file("urls.txt")
    if not urls:
        logger.error("‚ùå –ù–µ—Ç URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return
    
    logger.info(f"üöÄ –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(urls)} —Ç–æ–≤–∞—Ä–æ–≤")
    
    # ===== –†–ê–£–ù–î 1: –ü–µ—Ä–≤–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤ =====
    logger.info("=" * 80)
    logger.info("üîµ –†–ê–£–ù–î 1: –ü–µ—Ä–≤–∏—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤")
    logger.info("=" * 80)
    logger.info(f"üìã –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï {len(urls)} —Ç–æ–≤–∞—Ä–æ–≤ –∏–∑ urls.txt")
    results = await pipeline.process_urls(urls)
    
    # –ü–æ–º–µ—á–∞–µ–º –≤—Å–µ —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞—É–Ω–¥–∞ 1
    for result in pipeline.results:
        if result.get('status') == 'success' and 'processed_by_model' not in result:
            result['processed_by_model'] = 'gpt-4o-mini (Primary Round 1)'
    
    pipeline.print_statistics()
    
    # ===== –†–ê–£–ù–î 2: –ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ—á–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ =====
    failed_urls = pipeline.get_failed_urls()
    if failed_urls:
        logger.info("")
        logger.info("=" * 80)
        logger.info(f"üü° –†–ê–£–ù–î 2: –ü–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(failed_urls)} –æ—à–∏–±–æ—á–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤")
        logger.info("=" * 80)
        logger.info(f"üìã URL –¥–ª—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏: {failed_urls[:3]}..." if len(failed_urls) > 3 else f"üìã URL –¥–ª—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏: {failed_urls}")
        
        # –û—á–∏—â–∞–µ–º –æ—à–∏–±–∫–∏ –¥–ª—è —ç—Ç–∏—Ö URL
        pipeline.clear_errors_for_urls(failed_urls)
        
        # –ü–µ—Ä–µ–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        retry_results = await pipeline.process_urls(failed_urls)
        
        # –ü–æ–º–µ—á–∞–µ–º –≤—Å–µ —É—Å–ø–µ—à–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞—É–Ω–¥–∞ 2
        for result in pipeline.results:
            if result.get('url') in failed_urls and result.get('status') == 'success' and 'processed_by_model' not in result:
                result['processed_by_model'] = 'claude-3-haiku (Fallback Round 2)'
        
        pipeline.print_statistics()
        
        # ===== –†–ê–£–ù–î 3: –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å GPT-4o –¥–ª—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –æ—à–∏–±–æ–∫ =====
        failed_urls_round_2 = pipeline.get_failed_urls()
        if failed_urls_round_2:
            logger.info("")
            logger.info("=" * 80)
            logger.info(f"üî¥ –†–ê–£–ù–î 3: –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å GPT-4o –¥–ª—è {len(failed_urls_round_2)} —Ç–æ–≤–∞—Ä–æ–≤")
            logger.info("=" * 80)
            logger.info(f"üìã URL –¥–ª—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏: {failed_urls_round_2[:3]}..." if len(failed_urls_round_2) > 3 else f"üìã URL –¥–ª—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∏: {failed_urls_round_2}")
            logger.info("üî• –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å GPT-4o –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏")
            
            # –û—á–∏—â–∞–µ–º –æ—à–∏–±–∫–∏ –¥–ª—è —ç—Ç–∏—Ö URL
            pipeline.clear_errors_for_urls(failed_urls_round_2)
            
            # ‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º Resilient Recovery –Ω–∞ GPT-4o –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞—É–Ω–¥–∞
            from openai import OpenAI
            openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            pipeline.processor.llm_recovery.llm = openai_client
            pipeline.processor.llm_recovery.model = "gpt-4o"
            logger.info("üî• Resilient Recovery –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω –Ω–∞ GPT-4o")
            
            # ‚úÖ –°–ú–Ø–ì–ß–ê–ï–ú –í–ê–õ–ò–î–ê–¶–ò–Æ –¥–ª—è Round 3 (–µ—Å–ª–∏ GPT-4o –Ω–µ –º–æ–∂–µ—Ç - –ø—Ä–∏–Ω–∏–º–∞–µ–º —á—Ç–æ –µ—Å—Ç—å)
            pipeline.processor.relaxed_validation = True
            logger.info("üîµ –í–∫–ª—é—á–µ–Ω–∞ –°–ú–Ø–ì–ß–ï–ù–ù–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø –¥–ª—è Round 3 (FAQ‚â•2, advantages‚â•2, HTML‚â•800 –±–∞–π—Ç)")
            
            # –ü–æ–º–µ—á–∞–µ–º —á—Ç–æ —ç—Ç–æ —Ä–∞—É–Ω–¥ —Å GPT-4o
            pipeline.processor.current_model = "gpt-4o"
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å –º–æ—â–Ω–æ–π –º–æ–¥–µ–ª—å—é
            final_results = await pipeline.process_urls(failed_urls_round_2)
            
            # –ü–æ–º–µ—á–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–æ–≥–æ —Ä–∞—É–Ω–¥–∞ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ GPT-4o
            for result in pipeline.results:
                if result.get('url') in failed_urls_round_2 and result.get('status') == 'success':
                    result['processed_by_model'] = 'gpt-4o (Resilient Recovery Round 3)'
            
            pipeline.print_statistics()
    
    # ===== –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê =====
    logger.info("")
    logger.info("=" * 80)
    logger.info("üèÅ –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û–°–õ–ï –í–°–ï–• –†–ê–£–ù–î–û–í")
    logger.info("=" * 80)
    pipeline.print_statistics()
    
    # ‚úÖ –ù–û–í–û–ï: –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ Smart Routing
    try:
        # –ü–æ–ª—É—á–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä SmartLLMClient –∏–∑ –ª—é–±–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
        if hasattr(pipeline.processor, 'content_generator') and hasattr(pipeline.processor.content_generator, 'llm'):
            llm_client = pipeline.processor.content_generator.llm
            
            # –í—ã–≤–æ–¥–∏–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            llm_client.print_stats()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ª–æ–≥-—Ñ–∞–π–ª
            stats = llm_client.get_stats()
            
            import json
            with open('llm_usage_stats.json', 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)
            
            logger.info("üìÅ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Smart Routing —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ llm_usage_stats.json")
        else:
            logger.warning("‚ö†Ô∏è SmartLLMClient –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
            
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–≤–µ—Å—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É Smart Routing: {e}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Excel
    if pipeline.results:
        main_file = "descriptions.xlsx"
        
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º {len(pipeline.results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Excel...")
        
        # ‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –û—á–∏—â–∞–µ–º —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä –∏ –¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        # –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –º–µ–∂–¥—É —Ä–∞—É–Ω–¥–∞–º–∏
        pipeline.exporter.results = []
        for result in pipeline.results:
            await pipeline.exporter.add_result(result)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –≤ —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä–µ –¥–ª—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏
        pipeline.exporter.output_file = main_file
        # –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª)
        export_result = await pipeline.exporter.export_all()
        
        if export_result.get('success'):
            logger.info(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞–Ω—ã –≤ {main_file}")
            logger.info(f"üìä –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(pipeline.results)} —Ç–æ–≤–∞—Ä–æ–≤")
        else:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {export_result.get('error', 'Unknown error')}")
    else:
        logger.warning("‚ö†Ô∏è –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
    
    # ===== –ò–¢–û–ì–û–í–û–ï –°–ê–ú–ú–ê–†–ò =====
    end_time = datetime.now()
    total_time = (end_time - start_time).total_seconds()
    avg_time_per_product = total_time / len(urls) if urls else 0
    
    # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –º–æ–¥–µ–ª—è–º
    model_stats = {}
    for r in pipeline.results:
        model = r.get('processed_by_model', 'unknown')
        model_stats[model] = model_stats.get(model, 0) + 1
    
    # –ü–æ–¥—Å—á–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö/–æ—à–∏–±–æ–∫
    successful_count = sum(1 for r in pipeline.results if r.get('status') == 'success')
    error_count = sum(1 for r in pipeline.results if r.get('status') == 'error')
    
    logger.info("")
    logger.info("")
    logger.info("‚ïî" + "‚ïê" * 98 + "‚ïó")
    logger.info("‚ïë" + " " * 35 + "üéØ –ò–¢–û–ì–û–í–û–ï –°–ê–ú–ú–ê–†–ò –û–ë–†–ê–ë–û–¢–ö–ò" + " " * 35 + "‚ïë")
    logger.info("‚ïö" + "‚ïê" * 98 + "‚ïù")
    logger.info("")
    logger.info("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–†–ê–ë–û–¢–ö–ò:")
    logger.info(f"   ‚îú‚îÄ –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {len(urls)}")
    logger.info(f"   ‚îú‚îÄ ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful_count} ({successful_count/len(urls)*100:.1f}%)" if urls else "   ‚îú‚îÄ ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: 0")
    logger.info(f"   ‚îî‚îÄ ‚ùå –û—à–∏–±–æ–∫: {error_count} ({error_count/len(urls)*100:.1f}%)" if urls else "   ‚îî‚îÄ ‚ùå –û—à–∏–±–æ–∫: 0")
    logger.info("")
    logger.info("‚è±Ô∏è  –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:")
    logger.info(f"   ‚îú‚îÄ –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {int(total_time//60)} –º–∏–Ω {int(total_time%60)} —Å–µ–∫ ({total_time:.2f} —Å–µ–∫)")
    logger.info(f"   ‚îú‚îÄ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Ç–æ–≤–∞—Ä: {int(avg_time_per_product//60)} –º–∏–Ω {int(avg_time_per_product%60)} —Å–µ–∫ ({avg_time_per_product:.2f} —Å–µ–∫)")
    logger.info(f"   ‚îî‚îÄ –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(urls)/total_time:.3f} —Ç–æ–≤–∞—Ä–æ–≤/—Å–µ–∫" if total_time > 0 else "   ‚îî‚îÄ –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏: N/A")
    logger.info("")
    logger.info("ü§ñ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï LLM –ú–û–î–ï–õ–ï–ô:")
    
    if model_stats:
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É (–æ—Ç –±–æ–ª—å—à–µ–≥–æ –∫ –º–µ–Ω—å—à–µ–º—É)
        sorted_models = sorted(model_stats.items(), key=lambda x: x[1], reverse=True)
        
        for i, (model, count) in enumerate(sorted_models):
            percentage = (count / successful_count * 100) if successful_count > 0 else 0
            is_last = (i == len(sorted_models) - 1)
            prefix = "   ‚îî‚îÄ" if is_last else "   ‚îú‚îÄ"
            
            # –í—ã–¥–µ–ª—è–µ–º –¥–æ—Ä–æ–≥—É—é –º–æ–¥–µ–ª—å GPT-4o
            if 'gpt-4o' in model.lower() and 'mini' not in model.lower():
                logger.info(f"{prefix} üí∞ {model}: {count} —Ç–æ–≤–∞—Ä–æ–≤ ({percentage:.1f}%) ‚ö†Ô∏è  –î–û–†–û–ì–ê–Ø –ú–û–î–ï–õ–¨!")
            elif 'claude' in model.lower():
                logger.info(f"{prefix} üü£ {model}: {count} —Ç–æ–≤–∞—Ä–æ–≤ ({percentage:.1f}%) - Fallback")
            elif 'mini' in model.lower():
                logger.info(f"{prefix} üíö {model}: {count} —Ç–æ–≤–∞—Ä–æ–≤ ({percentage:.1f}%) - –û—Å–Ω–æ–≤–Ω–∞—è (–¥–µ—à–µ–≤–æ)")
            else:
                logger.info(f"{prefix} ü§ñ {model}: {count} —Ç–æ–≤–∞—Ä–æ–≤ ({percentage:.1f}%)")
    else:
        logger.info("   ‚îî‚îÄ –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –º–æ–¥–µ–ª—è—Ö")
    
    logger.info("")
    logger.info("=" * 100)
    logger.info(f"‚úÖ –û–ë–†–ê–ë–û–¢–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Excel")
    logger.info("=" * 100)
    logger.info("")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
